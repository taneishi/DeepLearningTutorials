
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Recurrent Neural Networks with Word Embeddings &#8212; DeepLearning 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinxdoc.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="LSTM Networks for Sentiment Analysis" href="lstm.html" />
    <link rel="prev" title="Hybrid Monte-Carlo Sampling" href="hmc.html" />
 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-168290-9']);
  _gaq.push(['_trackPageview']);
</script>

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="lstm.html" title="LSTM Networks for Sentiment Analysis"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="hmc.html" title="Hybrid Monte-Carlo Sampling"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Recurrent Neural Networks with Word Embeddings</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="recurrent-neural-networks-with-word-embeddings">
<span id="rnnslu"></span><h1>Recurrent Neural Networks with Word Embeddings<a class="headerlink" href="#recurrent-neural-networks-with-word-embeddings" title="Permalink to this heading">¶</a></h1>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h2>
<p>In this tutorial, you will learn how to:</p>
<ul class="simple">
<li><p>learn <strong>Word Embeddings</strong></p></li>
<li><p>using <strong>Recurrent Neural Networks</strong> architectures</p></li>
<li><p>with <strong>Context Windows</strong></p></li>
</ul>
<p>in order to perform Semantic Parsing / Slot-Filling (Spoken Language Understanding)</p>
</section>
<section id="code-citations-contact">
<h2>Code - Citations - Contact<a class="headerlink" href="#code-citations-contact" title="Permalink to this heading">¶</a></h2>
<section id="code">
<h3>Code<a class="headerlink" href="#code" title="Permalink to this heading">¶</a></h3>
<p>Directly running experiments is also possible using this <a class="reference external" href="https://github.com/mesnilgr/is13">github repository</a>.</p>
</section>
<section id="papers">
<h3>Papers<a class="headerlink" href="#papers" title="Permalink to this heading">¶</a></h3>
<p>If you use this tutorial, cite the following papers:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/RNNSpokenLanguage2013.pdf">[pdf]</a> Grégoire Mesnil, Xiaodong He, Li Deng and Yoshua Bengio. Investigation of Recurrent-Neural-Network Architectures and Learning Methods for Spoken Language Understanding. Interspeech, 2013.</p></li>
<li><p><a class="reference external" href="http://research.microsoft.com/en-us/people/gokhant/0000019.pdf">[pdf]</a> Gokhan Tur, Dilek Hakkani-Tur and Larry Heck. What is left to be understood in ATIS?</p></li>
<li><p><a class="reference external" href="http://lia.univ-avignon.fr/fileadmin/documents/Users/Intranet/fich_art/997-Interspeech2007.pdf">[pdf]</a> Christian Raymond and Giuseppe Riccardi. Generative and discriminative algorithms for spoken language understanding. Interspeech, 2007.</p></li>
<li><p><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/nips2012_deep_workshop_theano_final.pdf">[pdf]</a> Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.</p></li>
<li><p><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf">[pdf]</a> Bergstra, James, Breuleux, Olivier, Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.</p></li>
</ul>
<p>Thank you!</p>
</section>
<section id="contact">
<h3>Contact<a class="headerlink" href="#contact" title="Permalink to this heading">¶</a></h3>
<p>Please email to
<code class="docutils literal notranslate"><span class="pre">Grégoire</span> <span class="pre">Mesnil</span> <span class="pre">(first-add-a-dot-last-add-at-gmail-add-a-dot-com)</span></code>
for any problem report or feedback. We will be glad to hear from you.</p>
</section>
</section>
<section id="task">
<h2>Task<a class="headerlink" href="#task" title="Permalink to this heading">¶</a></h2>
<p>The Slot-Filling (Spoken Language Understanding) consists in assigning a label
to each word given a sentence. It’s a classification task.</p>
</section>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">¶</a></h2>
<p>An old and small benchmark for this task is the ATIS (Airline Travel Information
System) dataset collected by DARPA. Here is a sentence (or utterance) example using the
<a class="reference external" href="http://en.wikipedia.org/wiki/Inside_Outside_Beginning">Inside Outside Beginning (IOB)</a> representation.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><strong>Input</strong> (words)</p></td>
<td><p>show</p></td>
<td><p>flights</p></td>
<td><p>from</p></td>
<td><p>Boston</p></td>
<td><p>to</p></td>
<td><p>New</p></td>
<td><p>York</p></td>
<td><p>today</p></td>
</tr>
<tr class="row-even"><td><p><strong>Output</strong> (labels)</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>B-dept</p></td>
<td><p>O</p></td>
<td><p>B-arr</p></td>
<td><p>I-arr</p></td>
<td><p>B-date</p></td>
</tr>
</tbody>
</table>
<p>The ATIS offical split contains 4,978/893 sentences for a total of 56,590/9,198
words (average sentence length is 15) in the train/test set.  The number of
classes (different slots) is 128 including the O label (NULL).</p>
<p>As <a class="reference external" href="http://research.microsoft.com/en-us/um/people/gzweig/Pubs/Interspeech2013RNNLU.pdf">Microsoft Research people</a>,
we deal with unseen words in the test set by marking any words with only one
single occurrence in the training set as <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code> and use this token to
represent those unseen words in the test set. As <a class="reference external" href="http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf">Ronan Collobert and colleagues</a>, we converted
sequences of numbers with the string <code class="docutils literal notranslate"><span class="pre">DIGIT</span></code> i.e. <code class="docutils literal notranslate"><span class="pre">1984</span></code> is converted to
<code class="docutils literal notranslate"><span class="pre">DIGITDIGITDIGITDIGIT</span></code>.</p>
<p>We split the official train set into a training and validation set that contain
respectively 80% and 20% of the official training sentences. <a class="reference external" href="http://research.microsoft.com/en-us/um/people/gzweig/Pubs/Interspeech2013RNNLU.pdf">Significant
performance improvement difference has to be greater than 0.6% in F1 measure at
the 95% level due to the small size of the dataset</a>.
For evaluation purpose, experiments have to report the following metrics:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://en.wikipedia.org/wiki/Precision_(information_retrieval)">Precision</a></p></li>
<li><p><a class="reference external" href="http://en.wikipedia.org/wiki/Recall_(information_retrieval)">Recall</a></p></li>
<li><p><a class="reference external" href="http://en.wikipedia.org/wiki/F1_score">F1 score</a></p></li>
</ul>
<p>We will use the <a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt">conlleval</a> PERL script to
measure the performance of our models.</p>
</section>
<section id="recurrent-neural-network-model">
<h2>Recurrent Neural Network Model<a class="headerlink" href="#recurrent-neural-network-model" title="Permalink to this heading">¶</a></h2>
<section id="raw-input-encoding">
<h3>Raw input encoding<a class="headerlink" href="#raw-input-encoding" title="Permalink to this heading">¶</a></h3>
<p>A token corresponds to a word. Each token in the ATIS vocabulary is associated to an index. Each sentence is a
array of indexes (<code class="docutils literal notranslate"><span class="pre">int32</span></code>). Then, each set (train, valid, test) is a list of arrays of indexes. A python
dictionary is defined for mapping the space of indexes to the space of words.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sentence</span>
<span class="go">array([383, 189,  13, 193, 208, 307, 195, 502, 260, 539,</span>
<span class="go">        7,  60,  72, 8, 350, 384], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">index2word</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">sentence</span><span class="p">)</span>
<span class="go">[&#39;please&#39;, &#39;find&#39;, &#39;a&#39;, &#39;flight&#39;, &#39;from&#39;, &#39;miami&#39;, &#39;florida&#39;,</span>
<span class="go">        &#39;to&#39;, &#39;las&#39;, &#39;vegas&#39;, &#39;&lt;UNK&gt;&#39;, &#39;arriving&#39;, &#39;before&#39;, &#39;DIGIT&#39;, &quot;o&#39;clock&quot;, &#39;pm&#39;]</span>
</pre></div>
</div>
<p>Same thing for labels corresponding to this particular sentence.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span>
<span class="go">array([126, 126, 126, 126, 126,  48,  50, 126,  78, 123,  81, 126,  15,</span>
<span class="go">        14,  89,  89], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">index2label</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
<span class="go">[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-fromloc.city_name&#39;, &#39;B-fromloc.state_name&#39;,</span>
<span class="go">        &#39;O&#39;, &#39;B-toloc.city_name&#39;, &#39;I-toloc.city_name&#39;, &#39;B-toloc.state_name&#39;,</span>
<span class="go">        &#39;O&#39;, &#39;B-arrive_time.time_relative&#39;, &#39;B-arrive_time.time&#39;,</span>
<span class="go">        &#39;I-arrive_time.time&#39;, &#39;I-arrive_time.time&#39;]</span>
</pre></div>
</div>
</section>
<section id="context-window">
<h3>Context window<a class="headerlink" href="#context-window" title="Permalink to this heading">¶</a></h3>
<p>Given a sentence i.e. an array of indexes, and a window size i.e. 1,3,5,…, we
need to convert each word in the sentence to a context window surrounding this
particular word. In details, we have:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">contextwin</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">win</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    win :: int corresponding to the size of the window</span>
<span class="sd">    given a list of indexes composing a sentence</span>

<span class="sd">    l :: array containing the word indexes</span>

<span class="sd">    it will return a list of list of indexes corresponding</span>
<span class="sd">    to context windows surrounding each word in the sentence</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">win</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">win</span> <span class="o">&gt;=</span> <span class="mi">1</span>
    <span class="n">l</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

    <span class="n">lpadded</span> <span class="o">=</span> <span class="n">win</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span> <span class="o">+</span> <span class="n">win</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="n">lpadded</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span> <span class="o">+</span> <span class="n">win</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">))]</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>The index <code class="docutils literal notranslate"><span class="pre">-1</span></code> corresponds to the <code class="docutils literal notranslate"><span class="pre">PADDING</span></code> index we insert at the
beginning/end of the sentence.</p>
<p>Here is a sample:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">array([0, 1, 2, 3, 4], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contextwin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">[[-1, 0, 1],</span>
<span class="go"> [ 0, 1, 2],</span>
<span class="go"> [ 1, 2, 3],</span>
<span class="go"> [ 2, 3, 4],</span>
<span class="go"> [ 3, 4,-1]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contextwin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="go">[[-1, -1, -1, 0, 1, 2, 3],</span>
<span class="go"> [-1, -1,  0, 1, 2, 3, 4],</span>
<span class="go"> [-1,  0,  1, 2, 3, 4,-1],</span>
<span class="go"> [ 0,  1,  2, 3, 4,-1,-1],</span>
<span class="go"> [ 1,  2,  3, 4,-1,-1,-1]]</span>
</pre></div>
</div>
<p>To summarize, we started with an array of indexes and ended with a matrix of
indexes. Each line corresponds to the context window surrounding this word.</p>
</section>
<section id="word-embeddings">
<h3>Word embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this heading">¶</a></h3>
<p>Once we have the sentence converted to context windows i.e. a matrix of indexes, we have to associate
these indexes to the embeddings (real-valued vector associated to each word).
Using Theano, it gives:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">theano</span><span class="o">,</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">tensor</span> <span class="k">as</span> <span class="n">T</span>

<span class="c1"># nv :: size of our vocabulary</span>
<span class="c1"># de :: dimension of the embedding space</span>
<span class="c1"># cs :: context window size</span>
<span class="n">nv</span><span class="p">,</span> <span class="n">de</span><span class="p">,</span> <span class="n">cs</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> \
    <span class="p">(</span><span class="n">nv</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">de</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span> <span class="c1"># add one for PADDING at the end</span>

<span class="n">idxs</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">imatrix</span><span class="p">()</span> <span class="c1"># as many columns as words in the context window and as many lines as words in the sentence</span>
<span class="n">x</span>    <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">idxs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">de</span><span class="o">*</span><span class="n">cs</span><span class="p">))</span>
</pre></div>
</div>
<p>The x symbolic variable corresponds to a matrix of shape (number of words in the
sentences, dimension of the embedding space X context window size).</p>
<p>Let’s compile a theano function to do so</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sample</span>
<span class="go">array([0, 1, 2, 3, 4], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csample</span> <span class="o">=</span> <span class="n">contextwin</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="go">[[-1, -1, -1, 0, 1, 2, 3],</span>
<span class="go"> [-1, -1,  0, 1, 2, 3, 4],</span>
<span class="go"> [-1,  0,  1, 2, 3, 4,-1],</span>
<span class="go"> [ 0,  1,  2, 3, 4,-1,-1],</span>
<span class="go"> [ 1,  2,  3, 4,-1,-1,-1]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">csample</span><span class="p">)</span>
<span class="go">array([[-0.08088442,  0.08458307,  0.05064092, ...,  0.06876887,</span>
<span class="go">        -0.06648078, -0.15192257],</span>
<span class="go">       [-0.08088442,  0.08458307,  0.05064092, ...,  0.11192625,</span>
<span class="go">         0.08745284,  0.04381778],</span>
<span class="go">       [-0.08088442,  0.08458307,  0.05064092, ..., -0.00937143,</span>
<span class="go">         0.10804889,  0.1247109 ],</span>
<span class="go">       [ 0.11038255, -0.10563177, -0.18760249, ..., -0.00937143,</span>
<span class="go">         0.10804889,  0.1247109 ],</span>
<span class="go">       [ 0.18738101,  0.14727569, -0.069544  , ..., -0.00937143,</span>
<span class="go">         0.10804889,  0.1247109 ]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">csample</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(5, 350)</span>
</pre></div>
</div>
<p>We now have a sequence (of length 5 which is corresponds to the length of the
sentence) of <strong>context window word embeddings</strong> which is easy to feed to a simple
recurrent neural network to iterate with.</p>
</section>
<section id="elman-recurrent-neural-network">
<h3>Elman recurrent neural network<a class="headerlink" href="#elman-recurrent-neural-network" title="Permalink to this heading">¶</a></h3>
<p>The followin (Elman) recurrent neural network (E-RNN) takes as input the current input
(time <code class="docutils literal notranslate"><span class="pre">t</span></code>) and the previous hiddent state (time <code class="docutils literal notranslate"><span class="pre">t-1</span></code>). Then it iterates.</p>
<p>In the previous section, we processed the input to fit this
sequential/temporal structure.  It consists in a matrix where the row <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to
the time step <code class="docutils literal notranslate"><span class="pre">t=0</span></code>, the row <code class="docutils literal notranslate"><span class="pre">1</span></code> corresponds to the time step  <code class="docutils literal notranslate"><span class="pre">t=1</span></code>, etc.</p>
<p>The <strong>parameters</strong> of the E-RNN to be learned are:</p>
<ul class="simple">
<li><p>the word embeddings (real-valued matrix)</p></li>
<li><p>the initial hidden state (real-value vector)</p></li>
<li><p>two matrices for the linear projection of the input <code class="docutils literal notranslate"><span class="pre">t</span></code> and the previous hidden layer state <code class="docutils literal notranslate"><span class="pre">t-1</span></code></p></li>
<li><p>(optional) bias. <a class="reference external" href="http://en.wikipedia.org/wiki/Occam's_razor">Recommendation</a>: don’t use it.</p></li>
<li><p>softmax classification layer on top</p></li>
</ul>
<p>The <strong>hyperparameters</strong> define the whole architecture:</p>
<ul class="simple">
<li><p>dimension of the word embedding</p></li>
<li><p>size of the vocabulary</p></li>
<li><p>number of hidden units</p></li>
<li><p>number of classes</p></li>
<li><p>random seed + way to initialize the model</p></li>
</ul>
<p>It gives the following code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNNSLU</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; elman neural net model &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="n">nc</span><span class="p">,</span> <span class="n">ne</span><span class="p">,</span> <span class="n">de</span><span class="p">,</span> <span class="n">cs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        nh :: dimension of the hidden layer</span>
<span class="sd">        nc :: number of classes</span>
<span class="sd">        ne :: number of word embeddings in the vocabulary</span>
<span class="sd">        de :: dimension of the word embeddings</span>
<span class="sd">        cs :: word window context size</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># parameters of the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;embeddings&#39;</span><span class="p">,</span>
                                 <span class="n">value</span><span class="o">=</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                                 <span class="p">(</span><span class="n">ne</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">de</span><span class="p">))</span>
                                 <span class="c1"># add one for padding at the end</span>
                                 <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wx</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;wx&#39;</span><span class="p">,</span>
                                <span class="n">value</span><span class="o">=</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                                <span class="p">(</span><span class="n">de</span> <span class="o">*</span> <span class="n">cs</span><span class="p">,</span> <span class="n">nh</span><span class="p">))</span>
                                <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wh</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;wh&#39;</span><span class="p">,</span>
                                <span class="n">value</span><span class="o">=</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                                <span class="p">(</span><span class="n">nh</span><span class="p">,</span> <span class="n">nh</span><span class="p">))</span>
                                <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span>
                               <span class="n">value</span><span class="o">=</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                               <span class="p">(</span><span class="n">nh</span><span class="p">,</span> <span class="n">nc</span><span class="p">))</span>
                               <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bh</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;bh&#39;</span><span class="p">,</span>
                                <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span>
                               <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nc</span><span class="p">,</span>
                               <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h0</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;h0&#39;</span><span class="p">,</span>
                                <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>

        <span class="c1"># bundle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">bh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h0</span><span class="p">]</span>
</pre></div>
</div>
<p>Then we integrate the way to build the input from the embedding matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="n">idxs</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">imatrix</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">idxs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">de</span><span class="o">*</span><span class="n">cs</span><span class="p">))</span>
        <span class="n">y_sentence</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s1">&#39;y_sentence&#39;</span><span class="p">)</span>  <span class="c1"># labels</span>
</pre></div>
</div>
<p>We use the scan operator to construct the recursion, works like a charm:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
        <span class="k">def</span> <span class="nf">recurrence</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">h_tm1</span><span class="p">):</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wx</span><span class="p">)</span>
                                 <span class="o">+</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h_tm1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wh</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bh</span><span class="p">)</span>
            <span class="n">s_t</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">h_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">]</span>

        <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">recurrence</span><span class="p">,</span>
                                <span class="n">sequences</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                                <span class="n">outputs_info</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">h0</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
                                <span class="n">n_steps</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">p_y_given_x_sentence</span> <span class="o">=</span> <span class="n">s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p_y_given_x_sentence</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Theano will then compute all the gradients automatically to maximize the log-likelihood:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="n">lr</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">)</span>

        <span class="n">sentence_nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_y_given_x_sentence</span><span class="p">)</span>
                               <span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y_sentence</span><span class="p">])</span>
        <span class="n">sentence_gradients</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">sentence_nll</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="n">sentence_updates</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">g</span><span class="p">)</span>
                                       <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span>
                                       <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">sentence_gradients</span><span class="p">))</span>
</pre></div>
</div>
<p>Next compile those functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">classify</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sentence_train</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">idxs</span><span class="p">,</span> <span class="n">y_sentence</span><span class="p">,</span> <span class="n">lr</span><span class="p">],</span>
                                              <span class="n">outputs</span><span class="o">=</span><span class="n">sentence_nll</span><span class="p">,</span>
                                              <span class="n">updates</span><span class="o">=</span><span class="n">sentence_updates</span><span class="p">)</span>
</pre></div>
</div>
<p>We keep the word embeddings on the unit sphere by normalizing them after each update:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[],</span>
                                         <span class="n">updates</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">:</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">/</span>
                                                  <span class="n">T</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                                                  <span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
                                                  <span class="o">.</span><span class="n">dimshuffle</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">)})</span>
</pre></div>
</div>
<p>And that’s it!</p>
</section>
</section>
<section id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this heading">¶</a></h2>
<p>With the previous defined functions, you can compare the predicted labels with
the true labels and compute some metrics. In this <a class="reference external" href="https://github.com/mesnilgr/is13">repo</a>, we build a wrapper around the <a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt">conlleval</a> PERL script.
It’s not trivial to compute those metrics due to the <a class="reference external" href="http://en.wikipedia.org/wiki/Inside_Outside_Beginning">Inside Outside Beginning
(IOB)</a> representation
i.e. a prediction is considered correct if the word-beginning <strong>and</strong> the
word-inside <strong>and</strong> the word-outside predictions are <strong>all</strong> correct.
Note that the extension is <span class="math notranslate nohighlight">\(txt\)</span> and you will have to change it to <span class="math notranslate nohighlight">\(pl\)</span>.</p>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this heading">¶</a></h2>
<section id="updates">
<h3>Updates<a class="headerlink" href="#updates" title="Permalink to this heading">¶</a></h3>
<p>For stochastic gradient descent (SGD) update, we consider the whole sentence as a mini-batch
and perform one update per sentence. It is possible to perform a pure SGD (contrary to mini-batch)
where the update is done on only one single word at a time.</p>
<p>After each iteration/update, we normalize the word embeddings to keep them on a unit sphere.</p>
</section>
<section id="stopping-criterion">
<h3>Stopping Criterion<a class="headerlink" href="#stopping-criterion" title="Permalink to this heading">¶</a></h3>
<p>Early-stopping on a validation set is our regularization technique:
the training is run for a given number of epochs (a single pass through the
whole dataset) and keep the best model along with respect to the F1 score
computed on the validation set after each epoch.</p>
</section>
<section id="hyper-parameter-selection">
<h3>Hyper-Parameter Selection<a class="headerlink" href="#hyper-parameter-selection" title="Permalink to this heading">¶</a></h3>
<p>Although there is interesting research/<a class="reference external" href="https://github.com/JasperSnoek/spearmint">code</a> on the topic of automatic
hyper-parameter selection, we use the <a class="reference external" href="http://en.wikipedia.org/wiki/KISS_principle">KISS</a> random search.</p>
<p>The following intervals can give you some starting point:</p>
<ul class="simple">
<li><p>learning rate : uniform([0.05,0.01])</p></li>
<li><p>window size : random value from {3,…,19}</p></li>
<li><p>number of hidden units : random value from {100,200}</p></li>
<li><p>embedding dimension : random value from {50,100}</p></li>
</ul>
</section>
</section>
<section id="running-the-code">
<h2>Running the Code<a class="headerlink" href="#running-the-code" title="Permalink to this heading">¶</a></h2>
<p>After downloading the data using <span class="math notranslate nohighlight">\(download.sh\)</span>, the user can then run the code by calling:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>code/rnnslu.py

<span class="o">(</span><span class="s1">&#39;NEW BEST: epoch&#39;</span>,<span class="w"> </span><span class="m">25</span>,<span class="w"> </span><span class="s1">&#39;valid F1&#39;</span>,<span class="w"> </span><span class="m">96</span>.84,<span class="w"> </span><span class="s1">&#39;best test F1&#39;</span>,<span class="w"> </span><span class="m">93</span>.79<span class="o">)</span>
<span class="o">[</span>learning<span class="o">]</span><span class="w"> </span>epoch<span class="w"> </span><span class="m">26</span><span class="w"> </span>&gt;&gt;<span class="w"> </span><span class="m">100</span>.00%<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">28</span>.76<span class="w"> </span><span class="o">(</span>sec<span class="o">)</span><span class="w"> </span>&lt;&lt;
<span class="o">[</span>learning<span class="o">]</span><span class="w"> </span>epoch<span class="w"> </span><span class="m">27</span><span class="w"> </span>&gt;&gt;<span class="w"> </span><span class="m">100</span>.00%<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">28</span>.76<span class="w"> </span><span class="o">(</span>sec<span class="o">)</span><span class="w"> </span>&lt;&lt;
...
<span class="o">(</span><span class="s1">&#39;BEST RESULT: epoch&#39;</span>,<span class="w"> </span><span class="m">57</span>,<span class="w"> </span><span class="s1">&#39;valid F1&#39;</span>,<span class="w"> </span><span class="m">97</span>.23,<span class="w"> </span><span class="s1">&#39;best test F1&#39;</span>,<span class="w"> </span><span class="m">94</span>.2,<span class="w"> </span><span class="s1">&#39;with the model&#39;</span>,<span class="w"> </span><span class="s1">&#39;rnnslu&#39;</span><span class="o">)</span>
</pre></div>
</div>
<section id="timing">
<h3>Timing<a class="headerlink" href="#timing" title="Permalink to this heading">¶</a></h3>
<p>Running experiments on ATIS using this <a class="reference external" href="https://github.com/mesnilgr/is13">repository</a>
will run one epoch in less than 40 seconds on i7 CPU 950 &#64; 3.07GHz using less than 200 Mo of RAM:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">0</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">34.48</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
</pre></div>
</div>
<p>After a few epochs, you obtain decent performance <strong>94.48 % of F1 score</strong>.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">28</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">96.61</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.19</span>
<span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">29</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">96.63</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.42</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">30</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.04</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">31</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">34.80</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
<span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">40</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">97.25</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.34</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">41</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.18</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">42</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">97.33</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.48</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">43</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.39</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">44</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.31</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="word-embedding-nearest-neighbors">
<h3>Word Embedding Nearest Neighbors<a class="headerlink" href="#word-embedding-nearest-neighbors" title="Permalink to this heading">¶</a></h3>
<p>We can check the k-nearest neighbors of the learned embeddings. L2 and
cosine distance gave the same results so we plot them for the cosine distance.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>atlanta</strong></p></th>
<th class="head"><p><strong>back</strong></p></th>
<th class="head"><p><strong>ap80</strong></p></th>
<th class="head"><p><strong>but</strong></p></th>
<th class="head"><p><strong>aircraft</strong></p></th>
<th class="head"><p><strong>business</strong></p></th>
<th class="head"><p><strong>a</strong></p></th>
<th class="head"><p><strong>august</strong></p></th>
<th class="head"><p><strong>actually</strong></p></th>
<th class="head"><p><strong>cheap</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>phoenix</p></td>
<td><p>live</p></td>
<td><p>ap57</p></td>
<td><p>if</p></td>
<td><p>plane</p></td>
<td><p>coach</p></td>
<td><p>people</p></td>
<td><p>september</p></td>
<td><p>provide</p></td>
<td><p>weekday</p></td>
</tr>
<tr class="row-odd"><td><p>denver</p></td>
<td><p>lives</p></td>
<td><p>ap</p></td>
<td><p>up</p></td>
<td><p>service</p></td>
<td><p>first</p></td>
<td><p>do</p></td>
<td><p>january</p></td>
<td><p>prices</p></td>
<td><p>weekdays</p></td>
</tr>
<tr class="row-even"><td><p>tacoma</p></td>
<td><p>both</p></td>
<td><p>connections</p></td>
<td><p>a</p></td>
<td><p>airplane</p></td>
<td><p>fourth</p></td>
<td><p>but</p></td>
<td><p>june</p></td>
<td><p>stop</p></td>
<td><p>am</p></td>
</tr>
<tr class="row-odd"><td><p>columbus</p></td>
<td><p>how</p></td>
<td><p>tomorrow</p></td>
<td><p>now</p></td>
<td><p>seating</p></td>
<td><p>thrift</p></td>
<td><p>numbers</p></td>
<td><p>december</p></td>
<td><p>number</p></td>
<td><p>early</p></td>
</tr>
<tr class="row-even"><td><p>seattle</p></td>
<td><p>me</p></td>
<td><p>before</p></td>
<td><p>amount</p></td>
<td><p>stand</p></td>
<td><p>tenth</p></td>
<td><p>abbreviation</p></td>
<td><p>november</p></td>
<td><p>flight</p></td>
<td><p>sfo</p></td>
</tr>
<tr class="row-odd"><td><p>minneapolis</p></td>
<td><p>out</p></td>
<td><p>earliest</p></td>
<td><p>more</p></td>
<td><p>that</p></td>
<td><p>second</p></td>
<td><p>if</p></td>
<td><p>april</p></td>
<td><p>there</p></td>
<td><p>milwaukee</p></td>
</tr>
<tr class="row-even"><td><p>pittsburgh</p></td>
<td><p>other</p></td>
<td><p>connect</p></td>
<td><p>abbreviation</p></td>
<td><p>on</p></td>
<td><p>fifth</p></td>
<td><p>up</p></td>
<td><p>july</p></td>
<td><p>serving</p></td>
<td><p>jfk</p></td>
</tr>
<tr class="row-odd"><td><p>ontario</p></td>
<td><p>plane</p></td>
<td><p>thrift</p></td>
<td><p>restrictions</p></td>
<td><p>turboprop</p></td>
<td><p>third</p></td>
<td><p>serve</p></td>
<td><p>jfk</p></td>
<td><p>thank</p></td>
<td><p>shortest</p></td>
</tr>
<tr class="row-even"><td><p>montreal</p></td>
<td><p>service</p></td>
<td><p>coach</p></td>
<td><p>mean</p></td>
<td><p>mean</p></td>
<td><p>twelfth</p></td>
<td><p>database</p></td>
<td><p>october</p></td>
<td><p>ticket</p></td>
<td><p>bwi</p></td>
</tr>
<tr class="row-odd"><td><p>philadelphia</p></td>
<td><p>fare</p></td>
<td><p>today</p></td>
<td><p>interested</p></td>
<td><p>amount</p></td>
<td><p>sixth</p></td>
<td><p>passengers</p></td>
<td><p>may</p></td>
<td><p>are</p></td>
<td><p>lastest</p></td>
</tr>
</tbody>
</table>
<p>As you can judge, the limited size of the vocabulary (about 500 words) gives us mitigated
performance. According to human judgement: some are good, some are bad.</p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="contents.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Recurrent Neural Networks with Word Embeddings</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#code-citations-contact">Code - Citations - Contact</a><ul>
<li><a class="reference internal" href="#code">Code</a></li>
<li><a class="reference internal" href="#papers">Papers</a></li>
<li><a class="reference internal" href="#contact">Contact</a></li>
</ul>
</li>
<li><a class="reference internal" href="#task">Task</a></li>
<li><a class="reference internal" href="#dataset">Dataset</a></li>
<li><a class="reference internal" href="#recurrent-neural-network-model">Recurrent Neural Network Model</a><ul>
<li><a class="reference internal" href="#raw-input-encoding">Raw input encoding</a></li>
<li><a class="reference internal" href="#context-window">Context window</a></li>
<li><a class="reference internal" href="#word-embeddings">Word embeddings</a></li>
<li><a class="reference internal" href="#elman-recurrent-neural-network">Elman recurrent neural network</a></li>
</ul>
</li>
<li><a class="reference internal" href="#evaluation">Evaluation</a></li>
<li><a class="reference internal" href="#training">Training</a><ul>
<li><a class="reference internal" href="#updates">Updates</a></li>
<li><a class="reference internal" href="#stopping-criterion">Stopping Criterion</a></li>
<li><a class="reference internal" href="#hyper-parameter-selection">Hyper-Parameter Selection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-the-code">Running the Code</a><ul>
<li><a class="reference internal" href="#timing">Timing</a></li>
<li><a class="reference internal" href="#word-embedding-nearest-neighbors">Word Embedding Nearest Neighbors</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="hmc.html"
                          title="previous chapter">Hybrid Monte-Carlo Sampling</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="lstm.html"
                          title="next chapter">LSTM Networks for Sentiment Analysis</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/rnnslu.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="lstm.html" title="LSTM Networks for Sentiment Analysis"
             >next</a> |</li>
        <li class="right" >
          <a href="hmc.html" title="Hybrid Monte-Carlo Sampling"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Recurrent Neural Networks with Word Embeddings</a></li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2008--2010, LISA lab.
      Last updated on May 11, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.1.3.
    </div>
<script type="text/javascript">
  (function() {
    var ga = document.createElement('script');
    ga.src = ('https:' == document.location.protocol ?
              'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    ga.setAttribute('async', 'true');
    document.documentElement.firstChild.appendChild(ga);
  })();
</script>

  </body>
</html>