
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Classifying MNIST digits using Logistic Regression &#8212; DeepLearning 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinxdoc.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multilayer Perceptron" href="mlp.html" />
    <link rel="prev" title="Getting Started" href="gettingstarted.html" />
 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-168290-9']);
  _gaq.push(['_trackPageview']);
</script>

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="mlp.html" title="Multilayer Perceptron"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="gettingstarted.html" title="Getting Started"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Classifying MNIST digits using Logistic Regression</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="classifying-mnist-digits-using-logistic-regression">
<span id="logreg"></span><span id="index-0"></span><h1>Classifying MNIST digits using Logistic Regression<a class="headerlink" href="#classifying-mnist-digits-using-logistic-regression" title="Permalink to this heading">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This sections assumes familiarity with the following Theano
concepts: <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables">shared variables</a> , <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/adding.html#adding-two-scalars">basic arithmetic ops</a> , <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/examples.html#computing-gradients">T.grad</a> ,
<a class="reference external" href="http://deeplearning.net/software/theano/library/config.html#config.floatX">floatX</a>. If you intend to run the code on GPU also read <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/using_gpu.html">GPU</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The code for this section is available for download <a class="reference external" href="http://deeplearning.net/tutorial/code/logistic_sgd.py">here</a>.</p>
</div>
<p>In this section, we show how Theano can be used to implement the most basic
classifier: the logistic regression. We start off with a quick primer of the
model, which serves both as a refresher but also to anchor the notation and
show how mathematical expressions are mapped onto Theano graphs.</p>
<p>In the deepest of machine learning traditions, this tutorial will tackle the exciting
problem of MNIST digit classification.</p>
<section id="the-model">
<h2>The Model<a class="headerlink" href="#the-model" title="Permalink to this heading">¶</a></h2>
<p>Logistic regression is a probabilistic, linear classifier. It is parametrized
by a weight matrix <span class="math notranslate nohighlight">\(W\)</span> and a bias vector <span class="math notranslate nohighlight">\(b\)</span>. Classification is
done by projecting an input vector onto a set of hyperplanes, each of which
corresponds to a class. The distance from the input to a hyperplane reflects
the probability that the input is a member of the corresponding class.</p>
<p>Mathematically, the probability that an input vector <span class="math notranslate nohighlight">\(x\)</span> is a member of a
class <span class="math notranslate nohighlight">\(i\)</span>, a value of a stochastic variable <span class="math notranslate nohighlight">\(Y\)</span>, can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(Y=i|x, W,b) &amp;= softmax_i(W x + b) \\
              &amp;= \frac {e^{W_i x + b_i}} {\sum_j e^{W_j x + b_j}}\end{split}\]</div>
<p>The model’s prediction <span class="math notranslate nohighlight">\(y_{pred}\)</span> is the class whose probability is maximal, specifically:</p>
<div class="math notranslate nohighlight">
\[y_{pred} = {\rm argmax}_i P(Y=i|x,W,b)\]</div>
<p>The code to do this in Theano is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="c1"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span>
            <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="c1"># initialize the biases b as a vector of n_out 0s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_out</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span>
            <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># symbolic expression for computing the matrix of class-membership</span>
        <span class="c1"># probabilities</span>
        <span class="c1"># Where:</span>
        <span class="c1"># W is a matrix where column-k represent the separation hyperplane for</span>
        <span class="c1"># class-k</span>
        <span class="c1"># x is a matrix where row-j  represents input training sample-j</span>
        <span class="c1"># b is a vector where element-k represent the free parameter of</span>
        <span class="c1"># hyperplane-k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># symbolic description of how to compute prediction as class whose</span>
        <span class="c1"># probability is maximal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Since the parameters of the model must maintain a persistent state throughout
training, we allocate shared variables for <span class="math notranslate nohighlight">\(W,b\)</span>. This declares them both
as being symbolic Theano variables, but also initializes their contents. The
dot and softmax operators are then used to compute the vector <span class="math notranslate nohighlight">\(P(Y|x,
W,b)\)</span>. The result <code class="docutils literal notranslate"><span class="pre">p_y_given_x</span></code> is a symbolic variable of vector-type.</p>
<p>To get the actual model prediction, we can use the <code class="docutils literal notranslate"><span class="pre">T.argmax</span></code> operator, which
will return the index at which <code class="docutils literal notranslate"><span class="pre">p_y_given_x</span></code> is maximal (i.e. the class with
maximum probability).</p>
<p>Now of course, the model we have defined so far does not do anything useful
yet, since its parameters are still in their initial state. The following
section will thus cover how to learn the optimal parameters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For a complete list of Theano ops, see: <a class="reference external" href="http://deeplearning.net/software/theano/library/tensor/basic.html#basic-tensor-functionality">list of ops</a></p>
</div>
</section>
<section id="defining-a-loss-function">
<h2>Defining a Loss Function<a class="headerlink" href="#defining-a-loss-function" title="Permalink to this heading">¶</a></h2>
<p>Learning optimal model parameters involves minimizing a loss function. In the
case of multi-class logistic regression, it is very common to use the negative
log-likelihood as the loss. This is equivalent to maximizing the likelihood of the
data set <span class="math notranslate nohighlight">\(\cal{D}\)</span> under the model parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>. Let
us first start by defining the likelihood <span class="math notranslate nohighlight">\(\cal{L}\)</span> and loss
<span class="math notranslate nohighlight">\(\ell\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{L} (\theta=\{W,b\}, \mathcal{D}) =
  \sum_{i=0}^{|\mathcal{D}|} \log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\
\ell (\theta=\{W,b\}, \mathcal{D}) = - \mathcal{L} (\theta=\{W,b\}, \mathcal{D})\end{split}\]</div>
<p>While entire books are dedicated to the topic of minimization, gradient
descent is by far the simplest method for minimizing arbitrary non-linear
functions. This tutorial will use the method of stochastic gradient method with
mini-batches (MSGD). See <a class="reference internal" href="gettingstarted.html#opt-sgd"><span class="std std-ref">Stochastic Gradient Descent</span></a> for more details.</p>
<p>The following Theano code defines the (symbolic) loss for a given minibatch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="c1"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</span>
        <span class="c1"># number of examples (call it n) in the minibatch</span>
        <span class="c1"># T.arange(y.shape[0]) is a symbolic vector which will contain</span>
        <span class="c1"># [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of</span>
        <span class="c1"># Log-Probabilities (call it LP) with one row per example and</span>
        <span class="c1"># one column per class LP[T.arange(y.shape[0]),y] is a vector</span>
        <span class="c1"># v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,</span>
        <span class="c1"># LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is</span>
        <span class="c1"># the mean (across minibatch examples) of the elements in v,</span>
        <span class="c1"># i.e., the mean log-likelihood across the minibatch.</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">)[</span><span class="n">T</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Even though the loss is formally defined as the <em>sum</em>, over the data set,
of individual error terms, in practice, we use the <em>mean</em> (<code class="docutils literal notranslate"><span class="pre">T.mean</span></code>)
in the code. This allows for the learning rate choice to be less dependent
of the minibatch size.</p>
</div>
</section>
<section id="creating-a-logisticregression-class">
<h2>Creating a LogisticRegression class<a class="headerlink" href="#creating-a-logisticregression-class" title="Permalink to this heading">¶</a></h2>
<p>We now have all the tools we need to define a <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class, which
encapsulates the basic behaviour of logistic regression. The code is very
similar to what we have covered so far, and should be self explanatory.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-class Logistic Regression Class</span>

<span class="sd">    The logistic regression is fully described by a weight matrix :math:`W`</span>
<span class="sd">    and bias vector :math:`b`. Classification is done by projecting data</span>
<span class="sd">    points onto a set of hyperplanes, the distance to which is used to</span>
<span class="sd">    determine a class membership probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Initialize the parameters of the logistic regression</span>

<span class="sd">        :type input: theano.tensor.TensorType</span>
<span class="sd">        :param input: symbolic variable that describes the input of the</span>
<span class="sd">                      architecture (one minibatch)</span>

<span class="sd">        :type n_in: int</span>
<span class="sd">        :param n_in: number of input units, the dimension of the space in</span>
<span class="sd">                     which the datapoints lie</span>

<span class="sd">        :type n_out: int</span>
<span class="sd">        :param n_out: number of output units, the dimension of the space in</span>
<span class="sd">                      which the labels lie</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># start-snippet-1</span>
        <span class="c1"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span>
            <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="c1"># initialize the biases b as a vector of n_out 0s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_out</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span>
            <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># symbolic expression for computing the matrix of class-membership</span>
        <span class="c1"># probabilities</span>
        <span class="c1"># Where:</span>
        <span class="c1"># W is a matrix where column-k represent the separation hyperplane for</span>
        <span class="c1"># class-k</span>
        <span class="c1"># x is a matrix where row-j  represents input training sample-j</span>
        <span class="c1"># b is a vector where element-k represent the free parameter of</span>
        <span class="c1"># hyperplane-k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># symbolic description of how to compute prediction as class whose</span>
        <span class="c1"># probability is maximal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># end-snippet-1</span>

        <span class="c1"># parameters of the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

        <span class="c1"># keep track of model input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>

    <span class="k">def</span> <span class="nf">negative_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the mean of the negative log-likelihood of the prediction</span>
<span class="sd">        of this model under a given target distribution.</span>

<span class="sd">        .. math::</span>

<span class="sd">            \frac{1}{|\mathcal{D}|} \mathcal{L} (\theta=\{W,b\}, \mathcal{D}) =</span>
<span class="sd">            \frac{1}{|\mathcal{D}|} \sum_{i=0}^{|\mathcal{D}|}</span>
<span class="sd">                \log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\</span>
<span class="sd">            \ell (\theta=\{W,b\}, \mathcal{D})</span>

<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label</span>

<span class="sd">        Note: we use the mean instead of the sum so that</span>
<span class="sd">              the learning rate is less dependent on the batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># start-snippet-2</span>
        <span class="c1"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</span>
        <span class="c1"># number of examples (call it n) in the minibatch</span>
        <span class="c1"># T.arange(y.shape[0]) is a symbolic vector which will contain</span>
        <span class="c1"># [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of</span>
        <span class="c1"># Log-Probabilities (call it LP) with one row per example and</span>
        <span class="c1"># one column per class LP[T.arange(y.shape[0]),y] is a vector</span>
        <span class="c1"># v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,</span>
        <span class="c1"># LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is</span>
        <span class="c1"># the mean (across minibatch examples) of the elements in v,</span>
        <span class="c1"># i.e., the mean log-likelihood across the minibatch.</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">)[</span><span class="n">T</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">])</span>
        <span class="c1"># end-snippet-2</span>

    <span class="k">def</span> <span class="nf">errors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a float representing the number of errors in the minibatch</span>
<span class="sd">        over the total number of examples of the minibatch ; zero one</span>
<span class="sd">        loss over the size of the minibatch</span>

<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check if y has same dimension of y_pred</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s1">&#39;y should have the same shape as self.y_pred&#39;</span><span class="p">,</span>
                <span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># check if y is of the correct datatype</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">):</span>
            <span class="c1"># the T.neq operator returns a vector of 0s and 1s, where 1</span>
            <span class="c1"># represents a mistake in prediction</span>
            <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">neq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
<p>We instantiate this class as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
    <span class="c1"># generate symbolic variables for input (x and y represent a</span>
    <span class="c1"># minibatch)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>  <span class="c1"># data, presented as rasterized images</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>  <span class="c1"># labels, presented as 1D vector of [int] labels</span>

    <span class="c1"># construct the logistic regression class</span>
    <span class="c1"># Each MNIST image has size 28*28</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

</pre></div>
</div>
<p>We start by allocating symbolic variables for the training inputs <span class="math notranslate nohighlight">\(x\)</span> and
their corresponding classes <span class="math notranslate nohighlight">\(y\)</span>. Note that <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are defined
outside the scope of the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> object. Since the class
requires the input to build its graph, it is passed as a parameter of the
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> function. This is useful in case you want to connect instances of
such classes to form a deep network. The output of one layer can be passed as
the input of the layer above. (This tutorial does not build a multi-layer
network, but this code will be reused in future tutorials that do.)</p>
<p>Finally, we define a (symbolic) <code class="docutils literal notranslate"><span class="pre">cost</span></code> variable to minimize, using the instance
method <code class="docutils literal notranslate"><span class="pre">classifier.negative_log_likelihood</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
    <span class="c1"># the cost we minimize during training is the negative log likelihood of</span>
    <span class="c1"># the model in symbolic format</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">x</span></code> is an implicit symbolic input to the definition of <code class="docutils literal notranslate"><span class="pre">cost</span></code>,
because the symbolic variables of <code class="docutils literal notranslate"><span class="pre">classifier</span></code> were defined in terms of <code class="docutils literal notranslate"><span class="pre">x</span></code>
at initialization.</p>
</section>
<section id="learning-the-model">
<h2>Learning the Model<a class="headerlink" href="#learning-the-model" title="Permalink to this heading">¶</a></h2>
<p>To implement MSGD in most programming languages (C/C++, Matlab, Python), one
would start by manually deriving the expressions for the gradient of the loss
with respect to the parameters: in this case <span class="math notranslate nohighlight">\(\partial{\ell}/\partial{W}\)</span>,
and <span class="math notranslate nohighlight">\(\partial{\ell}/\partial{b}\)</span>, This can get pretty tricky for complex
models, as expressions for <span class="math notranslate nohighlight">\(\partial{\ell}/\partial{\theta}\)</span> can get
fairly complex, especially when taking into account problems of numerical
stability.</p>
<p>With Theano, this work is greatly simplified. It performs
automatic differentiation and applies certain math transforms to improve
numerical stability.</p>
<p>To get the gradients <span class="math notranslate nohighlight">\(\partial{\ell}/\partial{W}\)</span> and
<span class="math notranslate nohighlight">\(\partial{\ell}/\partial{b}\)</span> in Theano, simply do the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">g_W</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
    <span class="n">g_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">g_W</span></code> and <code class="docutils literal notranslate"><span class="pre">g_b</span></code> are symbolic variables, which can be used as part
of a computation graph. The function <code class="docutils literal notranslate"><span class="pre">train_model</span></code>, which performs one step
of gradient descent, can then be defined as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># specify how to update the parameters of the model as a list of</span>
    <span class="c1"># (variable, update expression) pairs.</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">[(</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_W</span><span class="p">),</span>
               <span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_b</span><span class="p">)]</span>

    <span class="c1"># compiling a Theano function `train_model` that returns the cost, but in</span>
    <span class="c1"># the same time updates the parameter of the model based on the rules</span>
    <span class="c1"># defined in `updates`</span>
    <span class="n">train_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span>
        <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
        <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">train_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">train_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">updates</span></code> is a list of pairs. In each pair, the first element is the symbolic
variable to be updated in the step, and the second element is the symbolic
function for calculating its new value. Similarly, <code class="docutils literal notranslate"><span class="pre">givens</span></code> is a dictionary
whose keys are symbolic variables and whose values specify
their replacements during the step. The function <code class="docutils literal notranslate"><span class="pre">train_model</span></code> is then defined such
that:</p>
<ul class="simple">
<li><p>the input is the mini-batch index <code class="docutils literal notranslate"><span class="pre">index</span></code> that, together with the batch
size (which is not an input since it is fixed) defines <span class="math notranslate nohighlight">\(x\)</span> with
corresponding labels <span class="math notranslate nohighlight">\(y\)</span></p></li>
<li><p>the return value is the cost/loss associated with the x, y defined by
the <code class="docutils literal notranslate"><span class="pre">index</span></code></p></li>
<li><p>on every function call, it will first replace <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> with the slices
from the training set specified by <code class="docutils literal notranslate"><span class="pre">index</span></code>. Then, it will evaluate the cost
associated with that minibatch and apply the operations defined by the
<code class="docutils literal notranslate"><span class="pre">updates</span></code> list.</p></li>
</ul>
<p>Each time <code class="docutils literal notranslate"><span class="pre">train_model(index)</span></code> is called, it will thus compute and return the
cost of a minibatch, while also performing a step of MSGD. The entire learning
algorithm thus consists in looping over all examples in the dataset, considering
all the examples in one minibatch at a time,
and repeatedly calling the <code class="docutils literal notranslate"><span class="pre">train_model</span></code> function.</p>
</section>
<section id="testing-the-model">
<h2>Testing the model<a class="headerlink" href="#testing-the-model" title="Permalink to this heading">¶</a></h2>
<p>As explained in <a class="reference internal" href="gettingstarted.html#opt-learn-classifier"><span class="std std-ref">Learning a Classifier</span></a>, when testing the model we are
interested in the number of misclassified examples (and not only in the likelihood).
The <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class therefore has an extra instance method, which
builds the symbolic graph for retrieving the number of misclassified examples in
each minibatch.</p>
<p>The code is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">errors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a float representing the number of errors in the minibatch</span>
<span class="sd">        over the total number of examples of the minibatch ; zero one</span>
<span class="sd">        loss over the size of the minibatch</span>

<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check if y has same dimension of y_pred</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s1">&#39;y should have the same shape as self.y_pred&#39;</span><span class="p">,</span>
                <span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># check if y is of the correct datatype</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">):</span>
            <span class="c1"># the T.neq operator returns a vector of 0s and 1s, where 1</span>
            <span class="c1"># represents a mistake in prediction</span>
            <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">neq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
<p>We then create a function <code class="docutils literal notranslate"><span class="pre">test_model</span></code> and a function <code class="docutils literal notranslate"><span class="pre">validate_model</span></code>,
which we can call to retrieve this value. As you will see shortly,
<code class="docutils literal notranslate"><span class="pre">validate_model</span></code> is key to our early-stopping implementation (see
<a class="reference internal" href="gettingstarted.html#opt-early-stopping"><span class="std std-ref">Early-Stopping</span></a>). These functions take a minibatch index and compute,
for the examples in that minibatch, the number that were misclassified by the
model. The only difference between them is that <code class="docutils literal notranslate"><span class="pre">test_model</span></code> draws its
minibatches from the testing set, while <code class="docutils literal notranslate"><span class="pre">validate_model</span></code> draws its from the
validation set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
    <span class="c1"># compiling a Theano function that computes the mistakes that are made by</span>
    <span class="c1"># the model on a minibatch</span>
    <span class="n">test_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
        <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">test_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">test_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">validate_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
        <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">valid_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">valid_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">)</span>

</pre></div>
</div>
</section>
<section id="putting-it-all-together">
<h2>Putting it All Together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this heading">¶</a></h2>
<p>The finished product is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This tutorial introduces logistic regression using Theano and stochastic</span>
<span class="sd">gradient descent.</span>

<span class="sd">Logistic regression is a probabilistic, linear classifier. It is parametrized</span>
<span class="sd">by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is</span>
<span class="sd">done by projecting data points onto a set of hyperplanes, the distance to</span>
<span class="sd">which is used to determine a class membership probability.</span>

<span class="sd">Mathematically, this can be written as:</span>

<span class="sd">.. math::</span>
<span class="sd">  P(Y=i|x, W,b) &amp;= softmax_i(W x + b) \\</span>
<span class="sd">                &amp;= \frac {e^{W_i x + b_i}} {\sum_j e^{W_j x + b_j}}</span>


<span class="sd">The output of the model or prediction is then done by taking the argmax of</span>
<span class="sd">the vector whose i&#39;th element is P(Y=i|x).</span>

<span class="sd">.. math::</span>

<span class="sd">  y_{pred} = argmax_i P(Y=i|x,W,b)</span>


<span class="sd">This tutorial presents a stochastic gradient descent optimization method</span>
<span class="sd">suitable for large datasets.</span>


<span class="sd">References:</span>

<span class="sd">    - textbooks: &quot;Pattern Recognition and Machine Learning&quot; -</span>
<span class="sd">                 Christopher M. Bishop, section 4.3.2</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="n">__docformat__</span> <span class="o">=</span> <span class="s1">&#39;restructedtext en&#39;</span>

<span class="kn">import</span> <span class="nn">six.moves.cPickle</span> <span class="k">as</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">timeit</span>

<span class="kn">import</span> <span class="nn">numpy</span>

<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">T</span>


<span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-class Logistic Regression Class</span>

<span class="sd">    The logistic regression is fully described by a weight matrix :math:`W`</span>
<span class="sd">    and bias vector :math:`b`. Classification is done by projecting data</span>
<span class="sd">    points onto a set of hyperplanes, the distance to which is used to</span>
<span class="sd">    determine a class membership probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Initialize the parameters of the logistic regression</span>

<span class="sd">        :type input: theano.tensor.TensorType</span>
<span class="sd">        :param input: symbolic variable that describes the input of the</span>
<span class="sd">                      architecture (one minibatch)</span>

<span class="sd">        :type n_in: int</span>
<span class="sd">        :param n_in: number of input units, the dimension of the space in</span>
<span class="sd">                     which the datapoints lie</span>

<span class="sd">        :type n_out: int</span>
<span class="sd">        :param n_out: number of output units, the dimension of the space in</span>
<span class="sd">                      which the labels lie</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># start-snippet-1</span>
        <span class="c1"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span>
            <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="c1"># initialize the biases b as a vector of n_out 0s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_out</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span>
            <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># symbolic expression for computing the matrix of class-membership</span>
        <span class="c1"># probabilities</span>
        <span class="c1"># Where:</span>
        <span class="c1"># W is a matrix where column-k represent the separation hyperplane for</span>
        <span class="c1"># class-k</span>
        <span class="c1"># x is a matrix where row-j  represents input training sample-j</span>
        <span class="c1"># b is a vector where element-k represent the free parameter of</span>
        <span class="c1"># hyperplane-k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># symbolic description of how to compute prediction as class whose</span>
        <span class="c1"># probability is maximal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># end-snippet-1</span>

        <span class="c1"># parameters of the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

        <span class="c1"># keep track of model input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>

    <span class="k">def</span> <span class="nf">negative_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the mean of the negative log-likelihood of the prediction</span>
<span class="sd">        of this model under a given target distribution.</span>

<span class="sd">        .. math::</span>

<span class="sd">            \frac{1}{|\mathcal{D}|} \mathcal{L} (\theta=\{W,b\}, \mathcal{D}) =</span>
<span class="sd">            \frac{1}{|\mathcal{D}|} \sum_{i=0}^{|\mathcal{D}|}</span>
<span class="sd">                \log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\</span>
<span class="sd">            \ell (\theta=\{W,b\}, \mathcal{D})</span>

<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label</span>

<span class="sd">        Note: we use the mean instead of the sum so that</span>
<span class="sd">              the learning rate is less dependent on the batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># start-snippet-2</span>
        <span class="c1"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</span>
        <span class="c1"># number of examples (call it n) in the minibatch</span>
        <span class="c1"># T.arange(y.shape[0]) is a symbolic vector which will contain</span>
        <span class="c1"># [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of</span>
        <span class="c1"># Log-Probabilities (call it LP) with one row per example and</span>
        <span class="c1"># one column per class LP[T.arange(y.shape[0]),y] is a vector</span>
        <span class="c1"># v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,</span>
        <span class="c1"># LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is</span>
        <span class="c1"># the mean (across minibatch examples) of the elements in v,</span>
        <span class="c1"># i.e., the mean log-likelihood across the minibatch.</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">)[</span><span class="n">T</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">])</span>
        <span class="c1"># end-snippet-2</span>

    <span class="k">def</span> <span class="nf">errors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a float representing the number of errors in the minibatch</span>
<span class="sd">        over the total number of examples of the minibatch ; zero one</span>
<span class="sd">        loss over the size of the minibatch</span>

<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check if y has same dimension of y_pred</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s1">&#39;y should have the same shape as self.y_pred&#39;</span><span class="p">,</span>
                <span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># check if y is of the correct datatype</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">):</span>
            <span class="c1"># the T.neq operator returns a vector of 0s and 1s, where 1</span>
            <span class="c1"># represents a mistake in prediction</span>
            <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">neq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; Loads the dataset</span>

<span class="sd">    :type dataset: string</span>
<span class="sd">    :param dataset: the path to the dataset (here MNIST)</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1">#############</span>
    <span class="c1"># LOAD DATA #</span>
    <span class="c1">#############</span>

    <span class="c1"># Download the MNIST dataset if it is not present</span>
    <span class="n">data_dir</span><span class="p">,</span> <span class="n">data_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data_dir</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="c1"># Check if dataset is in the data directory.</span>
        <span class="n">new_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
            <span class="s2">&quot;..&quot;</span><span class="p">,</span>
            <span class="s2">&quot;data&quot;</span><span class="p">,</span>
            <span class="n">dataset</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">new_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">data_file</span> <span class="o">==</span> <span class="s1">&#39;mnist.pkl.gz&#39;</span><span class="p">:</span>
            <span class="n">dataset</span> <span class="o">=</span> <span class="n">new_path</span>

    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span> <span class="ow">and</span> <span class="n">data_file</span> <span class="o">==</span> <span class="s1">&#39;mnist.pkl.gz&#39;</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="n">urllib</span>
        <span class="n">origin</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz&#39;</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Downloading data from </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">origin</span><span class="p">)</span>
        <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">origin</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;... loading data&#39;</span><span class="p">)</span>

    <span class="c1"># Load the dataset</span>
    <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin1&#39;</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="c1"># train_set, valid_set, test_set format: tuple(input, target)</span>
    <span class="c1"># input is a numpy.ndarray of 2 dimensions (a matrix)</span>
    <span class="c1"># where each row corresponds to an example. target is a</span>
    <span class="c1"># numpy.ndarray of 1 dimension (vector) that has the same length as</span>
    <span class="c1"># the number of rows in the input. It should give the target</span>
    <span class="c1"># to the example with the same index in the input.</span>

    <span class="k">def</span> <span class="nf">shared_dataset</span><span class="p">(</span><span class="n">data_xy</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that loads the dataset into shared variables</span>

<span class="sd">        The reason we store our dataset in shared variables is to allow</span>
<span class="sd">        Theano to copy it into the GPU memory (when code is run on GPU).</span>
<span class="sd">        Since copying data into the GPU is slow, copying a minibatch everytime</span>
<span class="sd">        is needed (the default behaviour if the data is not in a shared</span>
<span class="sd">        variable) would lead to a large decrease in performance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span> <span class="o">=</span> <span class="n">data_xy</span>
        <span class="n">shared_x</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span>
                                               <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
                                 <span class="n">borrow</span><span class="o">=</span><span class="n">borrow</span><span class="p">)</span>
        <span class="n">shared_y</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data_y</span><span class="p">,</span>
                                               <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
                                 <span class="n">borrow</span><span class="o">=</span><span class="n">borrow</span><span class="p">)</span>
        <span class="c1"># When storing data on the GPU it has to be stored as floats</span>
        <span class="c1"># therefore we will store the labels as ``floatX`` as well</span>
        <span class="c1"># (``shared_y`` does exactly that). But during our computations</span>
        <span class="c1"># we need them as ints (we use labels as index, and if they are</span>
        <span class="c1"># floats it doesn&#39;t make sense) therefore instead of returning</span>
        <span class="c1"># ``shared_y`` we will have to cast it to int. This little hack</span>
        <span class="c1"># lets ous get around this issue</span>
        <span class="k">return</span> <span class="n">shared_x</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">shared_y</span><span class="p">,</span> <span class="s1">&#39;int32&#39;</span><span class="p">)</span>

    <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span> <span class="o">=</span> <span class="n">shared_dataset</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span>
    <span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span> <span class="o">=</span> <span class="n">shared_dataset</span><span class="p">(</span><span class="n">valid_set</span><span class="p">)</span>
    <span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span> <span class="o">=</span> <span class="n">shared_dataset</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

    <span class="n">rval</span> <span class="o">=</span> <span class="p">[(</span><span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span><span class="p">),</span> <span class="p">(</span><span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span><span class="p">),</span>
            <span class="p">(</span><span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">rval</span>


<span class="k">def</span> <span class="nf">sgd_optimization_mnist</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.13</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                           <span class="n">dataset</span><span class="o">=</span><span class="s1">&#39;mnist.pkl.gz&#39;</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">600</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Demonstrate stochastic gradient descent optimization of a log-linear</span>
<span class="sd">    model</span>

<span class="sd">    This is demonstrated on MNIST.</span>

<span class="sd">    :type learning_rate: float</span>
<span class="sd">    :param learning_rate: learning rate used (factor for the stochastic</span>
<span class="sd">                          gradient)</span>

<span class="sd">    :type n_epochs: int</span>
<span class="sd">    :param n_epochs: maximal number of epochs to run the optimizer</span>

<span class="sd">    :type dataset: string</span>
<span class="sd">    :param dataset: the path of the MNIST dataset file from</span>
<span class="sd">                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">datasets</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># compute number of minibatches for training, validation and testing</span>
    <span class="n">n_train_batches</span> <span class="o">=</span> <span class="n">train_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">n_valid_batches</span> <span class="o">=</span> <span class="n">valid_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">n_test_batches</span> <span class="o">=</span> <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>

    <span class="c1">######################</span>
    <span class="c1"># BUILD ACTUAL MODEL #</span>
    <span class="c1">######################</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;... building the model&#39;</span><span class="p">)</span>

    <span class="c1"># allocate symbolic variables for the data</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">()</span>  <span class="c1"># index to a [mini]batch</span>

    <span class="c1"># generate symbolic variables for input (x and y represent a</span>
    <span class="c1"># minibatch)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>  <span class="c1"># data, presented as rasterized images</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>  <span class="c1"># labels, presented as 1D vector of [int] labels</span>

    <span class="c1"># construct the logistic regression class</span>
    <span class="c1"># Each MNIST image has size 28*28</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># the cost we minimize during training is the negative log likelihood of</span>
    <span class="c1"># the model in symbolic format</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># compiling a Theano function that computes the mistakes that are made by</span>
    <span class="c1"># the model on a minibatch</span>
    <span class="n">test_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
        <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">test_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">test_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">validate_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
        <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">valid_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">valid_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># compute the gradient of cost with respect to theta = (W,b)</span>
    <span class="n">g_W</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
    <span class="n">g_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># start-snippet-3</span>
    <span class="c1"># specify how to update the parameters of the model as a list of</span>
    <span class="c1"># (variable, update expression) pairs.</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">[(</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_W</span><span class="p">),</span>
               <span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_b</span><span class="p">)]</span>

    <span class="c1"># compiling a Theano function `train_model` that returns the cost, but in</span>
    <span class="c1"># the same time updates the parameter of the model based on the rules</span>
    <span class="c1"># defined in `updates`</span>
    <span class="n">train_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span>
        <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
        <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">train_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
            <span class="n">y</span><span class="p">:</span> <span class="n">train_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="c1"># end-snippet-3</span>

    <span class="c1">###############</span>
    <span class="c1"># TRAIN MODEL #</span>
    <span class="c1">###############</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;... training the model&#39;</span><span class="p">)</span>
    <span class="c1"># early-stopping parameters</span>
    <span class="n">patience</span> <span class="o">=</span> <span class="mi">5000</span>  <span class="c1"># look as this many examples regardless</span>
    <span class="n">patience_increase</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># wait this much longer when a new best is</span>
                                  <span class="c1"># found</span>
    <span class="n">improvement_threshold</span> <span class="o">=</span> <span class="mf">0.995</span>  <span class="c1"># a relative improvement of this much is</span>
                                  <span class="c1"># considered significant</span>
    <span class="n">validation_frequency</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">,</span> <span class="n">patience</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
                                  <span class="c1"># go through this many</span>
                                  <span class="c1"># minibatche before checking the network</span>
                                  <span class="c1"># on the validation set; in this case we</span>
                                  <span class="c1"># check every epoch</span>

    <span class="n">best_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>

    <span class="n">done_looping</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">&lt;</span> <span class="n">n_epochs</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done_looping</span><span class="p">):</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">minibatch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">):</span>

            <span class="n">minibatch_avg_cost</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">minibatch_index</span><span class="p">)</span>
            <span class="c1"># iteration number</span>
            <span class="nb">iter</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_train_batches</span> <span class="o">+</span> <span class="n">minibatch_index</span>

            <span class="k">if</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">validation_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># compute zero-one loss on validation set</span>
                <span class="n">validation_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">validate_model</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                                     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_valid_batches</span><span class="p">)]</span>
                <span class="n">this_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">validation_losses</span><span class="p">)</span>

                <span class="nb">print</span><span class="p">(</span>
                    <span class="s1">&#39;epoch </span><span class="si">%i</span><span class="s1">, minibatch </span><span class="si">%i</span><span class="s1">/</span><span class="si">%i</span><span class="s1">, validation error </span><span class="si">%f</span><span class="s1"> </span><span class="si">%%</span><span class="s1">&#39;</span> <span class="o">%</span>
                    <span class="p">(</span>
                        <span class="n">epoch</span><span class="p">,</span>
                        <span class="n">minibatch_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="n">n_train_batches</span><span class="p">,</span>
                        <span class="n">this_validation_loss</span> <span class="o">*</span> <span class="mf">100.</span>
                    <span class="p">)</span>
                <span class="p">)</span>

                <span class="c1"># if we got the best validation score until now</span>
                <span class="k">if</span> <span class="n">this_validation_loss</span> <span class="o">&lt;</span> <span class="n">best_validation_loss</span><span class="p">:</span>
                    <span class="c1">#improve patience if loss improvement is good enough</span>
                    <span class="k">if</span> <span class="n">this_validation_loss</span> <span class="o">&lt;</span> <span class="n">best_validation_loss</span> <span class="o">*</span>  \
                       <span class="n">improvement_threshold</span><span class="p">:</span>
                        <span class="n">patience</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">patience</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">*</span> <span class="n">patience_increase</span><span class="p">)</span>

                    <span class="n">best_validation_loss</span> <span class="o">=</span> <span class="n">this_validation_loss</span>
                    <span class="c1"># test it on the test set</span>

                    <span class="n">test_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_model</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_batches</span><span class="p">)]</span>
                    <span class="n">test_score</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_losses</span><span class="p">)</span>

                    <span class="nb">print</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="s1">&#39;     epoch </span><span class="si">%i</span><span class="s1">, minibatch </span><span class="si">%i</span><span class="s1">/</span><span class="si">%i</span><span class="s1">, test error of&#39;</span>
                            <span class="s1">&#39; best model </span><span class="si">%f</span><span class="s1"> </span><span class="si">%%</span><span class="s1">&#39;</span>
                        <span class="p">)</span> <span class="o">%</span>
                        <span class="p">(</span>
                            <span class="n">epoch</span><span class="p">,</span>
                            <span class="n">minibatch_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="n">n_train_batches</span><span class="p">,</span>
                            <span class="n">test_score</span> <span class="o">*</span> <span class="mf">100.</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

                    <span class="c1"># save the best model</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;best_model.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">patience</span> <span class="o">&lt;=</span> <span class="nb">iter</span><span class="p">:</span>
                <span class="n">done_looping</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>

    <span class="n">end_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="s1">&#39;Optimization complete with best validation score of </span><span class="si">%f</span><span class="s1"> </span><span class="si">%%</span><span class="s1">,&#39;</span>
            <span class="s1">&#39;with test performance </span><span class="si">%f</span><span class="s1"> </span><span class="si">%%</span><span class="s1">&#39;</span>
        <span class="p">)</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">best_validation_loss</span> <span class="o">*</span> <span class="mf">100.</span><span class="p">,</span> <span class="n">test_score</span> <span class="o">*</span> <span class="mf">100.</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The code run for </span><span class="si">%d</span><span class="s1"> epochs, with </span><span class="si">%f</span><span class="s1"> epochs/sec&#39;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">epoch</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">*</span> <span class="n">epoch</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">((</span><span class="s1">&#39;The code for file &#39;</span> <span class="o">+</span>
           <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
           <span class="s1">&#39; ran for </span><span class="si">%.1f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">((</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))),</span> <span class="n">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">predict</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An example of how to load a trained model and use it</span>
<span class="sd">    to predict labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># load the saved model</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;best_model.pkl&#39;</span><span class="p">))</span>

    <span class="c1"># compile a predictor function</span>
    <span class="n">predict_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">classifier</span><span class="o">.</span><span class="n">input</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="c1"># We can test it on some examples from test test</span>
    <span class="n">dataset</span><span class="o">=</span><span class="s1">&#39;mnist.pkl.gz&#39;</span>
    <span class="n">datasets</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">test_set_x</span> <span class="o">=</span> <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span>

    <span class="n">predicted_values</span> <span class="o">=</span> <span class="n">predict_model</span><span class="p">(</span><span class="n">test_set_x</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted values for the first 10 examples in test set:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">predicted_values</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">sgd_optimization_mnist</span><span class="p">()</span>
</pre></div>
</div>
<p>The user can learn to classify MNIST digits with SGD logistic regression, by typing, from
within the DeepLearningTutorials folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>code/logistic_sgd.py
</pre></div>
</div>
<p>The output one should expect is of the form:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>...
epoch<span class="w"> </span><span class="m">72</span>,<span class="w"> </span>minibatch<span class="w"> </span><span class="m">83</span>/83,<span class="w"> </span>validation<span class="w"> </span>error<span class="w"> </span><span class="m">7</span>.510417<span class="w"> </span>%
<span class="w">     </span>epoch<span class="w"> </span><span class="m">72</span>,<span class="w"> </span>minibatch<span class="w"> </span><span class="m">83</span>/83,<span class="w"> </span><span class="nb">test</span><span class="w"> </span>error<span class="w"> </span>of<span class="w"> </span>best<span class="w"> </span>model<span class="w"> </span><span class="m">7</span>.510417<span class="w"> </span>%
epoch<span class="w"> </span><span class="m">73</span>,<span class="w"> </span>minibatch<span class="w"> </span><span class="m">83</span>/83,<span class="w"> </span>validation<span class="w"> </span>error<span class="w"> </span><span class="m">7</span>.500000<span class="w"> </span>%
<span class="w">     </span>epoch<span class="w"> </span><span class="m">73</span>,<span class="w"> </span>minibatch<span class="w"> </span><span class="m">83</span>/83,<span class="w"> </span><span class="nb">test</span><span class="w"> </span>error<span class="w"> </span>of<span class="w"> </span>best<span class="w"> </span>model<span class="w"> </span><span class="m">7</span>.489583<span class="w"> </span>%
Optimization<span class="w"> </span><span class="nb">complete</span><span class="w"> </span>with<span class="w"> </span>best<span class="w"> </span>validation<span class="w"> </span>score<span class="w"> </span>of<span class="w"> </span><span class="m">7</span>.500000<span class="w"> </span>%,with<span class="w"> </span><span class="nb">test</span><span class="w"> </span>performance<span class="w"> </span><span class="m">7</span>.489583<span class="w"> </span>%
The<span class="w"> </span>code<span class="w"> </span>run<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="m">74</span><span class="w"> </span>epochs,<span class="w"> </span>with<span class="w"> </span><span class="m">1</span>.936983<span class="w"> </span>epochs/sec
</pre></div>
</div>
<p>On an Intel(R) Core(TM)2 Duo CPU E8400 &#64; 3.00 Ghz  the code runs with
approximately 1.936 epochs/sec and it took 75 epochs to reach a test
error of 7.489%. On the GPU the code does almost 10.0 epochs/sec. For this
instance we used a batch size of 600.</p>
</section>
<section id="prediction-using-a-trained-model">
<h2>Prediction Using a Trained Model<a class="headerlink" href="#prediction-using-a-trained-model" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">sgd_optimization_mnist</span></code> serialize and pickle the model each time new
lowest validation error is reached. We can reload this model and predict
labels of new data. <code class="docutils literal notranslate"><span class="pre">predict</span></code> function shows an example of how
this could be done.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An example of how to load a trained model and use it</span>
<span class="sd">    to predict labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># load the saved model</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;best_model.pkl&#39;</span><span class="p">))</span>

    <span class="c1"># compile a predictor function</span>
    <span class="n">predict_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">classifier</span><span class="o">.</span><span class="n">input</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="c1"># We can test it on some examples from test test</span>
    <span class="n">dataset</span><span class="o">=</span><span class="s1">&#39;mnist.pkl.gz&#39;</span>
    <span class="n">datasets</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">test_set_x</span> <span class="o">=</span> <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span>

    <span class="n">predicted_values</span> <span class="o">=</span> <span class="n">predict_model</span><span class="p">(</span><span class="n">test_set_x</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted values for the first 10 examples in test set:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">predicted_values</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="f1" role="note">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>For smaller datasets and simpler models, more sophisticated descent
algorithms can be more effective. The sample code
<a class="reference external" href="http://deeplearning.net/tutorial/code/logistic_cg.py">logistic_cg.py</a>
demonstrates how to use SciPy’s conjugate gradient solver with Theano
on the logistic regression task.</p>
</aside>
</aside>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="contents.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Classifying MNIST digits using Logistic Regression</a><ul>
<li><a class="reference internal" href="#the-model">The Model</a></li>
<li><a class="reference internal" href="#defining-a-loss-function">Defining a Loss Function</a></li>
<li><a class="reference internal" href="#creating-a-logisticregression-class">Creating a LogisticRegression class</a></li>
<li><a class="reference internal" href="#learning-the-model">Learning the Model</a></li>
<li><a class="reference internal" href="#testing-the-model">Testing the model</a></li>
<li><a class="reference internal" href="#putting-it-all-together">Putting it All Together</a></li>
<li><a class="reference internal" href="#prediction-using-a-trained-model">Prediction Using a Trained Model</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="gettingstarted.html"
                          title="previous chapter">Getting Started</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="mlp.html"
                          title="next chapter">Multilayer Perceptron</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/logreg.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="mlp.html" title="Multilayer Perceptron"
             >next</a> |</li>
        <li class="right" >
          <a href="gettingstarted.html" title="Getting Started"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Classifying MNIST digits using Logistic Regression</a></li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2008--2010, LISA lab.
      Last updated on May 11, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.1.3.
    </div>
<script type="text/javascript">
  (function() {
    var ga = document.createElement('script');
    ga.src = ('https:' == document.location.protocol ?
              'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    ga.setAttribute('async', 'true');
    document.documentElement.firstChild.appendChild(ga);
  })();
</script>

  </body>
</html>