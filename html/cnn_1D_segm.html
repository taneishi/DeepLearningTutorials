
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Network for 1D segmentation &#8212; DeepLearning 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinxdoc.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="U-Net" href="unet.html" />
    <link rel="prev" title="Fully Convolutional Networks (FCN) for 2D segmentation" href="fcn_2D_segm.html" />
 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-168290-9']);
  _gaq.push(['_trackPageview']);
</script>

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="unet.html" title="U-Net"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="fcn_2D_segm.html" title="Fully Convolutional Networks (FCN) for 2D segmentation"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Network for 1D segmentation</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="network-for-1d-segmentation">
<span id="cnn-1d-segm"></span><h1>Network for 1D segmentation<a class="headerlink" href="#network-for-1d-segmentation" title="Permalink to this heading">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section assumes the reader has already read through <a class="reference internal" href="lenet.html"><span class="doc">Convolutional Neural Networks (LeNet)</span></a> for
convolutional networks motivation and <a class="reference internal" href="fcn_2D_segm.html"><span class="doc">Fully Convolutional Networks (FCN) for 2D segmentation</span></a> for segmentation
standard network.</p>
</div>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h2>
<p>The fundamental notions behind segmentation have been explained in <a class="reference internal" href="fcn_2D_segm.html"><span class="doc">Fully Convolutional Networks (FCN) for 2D segmentation</span></a>.
A particularity here is that some of these notions will be applied to 1D
segmentation. However, almost every Lasagne layer used for 2D segmentation have
their respective 1D layer, so the implementation would look alike if the same
model was used.</p>
</section>
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this heading">¶</a></h2>
<p>The <a class="reference external" href="https://bigbrain.loris.ca/main.php">BigBrain</a> dataset is a 3D ultra-high resolution model of the brain reconstructed from 2D sections.
We are interested in the outer part of the brain, the cortex.
More precisely, we are interested in segmenting the 6 different layers of the cortex in 3D.
Creating an expertly labelled training dataset with each 2D section (shown in figure 1) is unfeasible. Instead of giving as input a 2D image of one section of the brain, we give as input 1D vectors with information from across the cortex, extracted from smaller portions of manually labelled cortex
as shown in Figure 2. The final dataset is not available yet, a preliminary version
is available <a class="reference external" href="https://drive.google.com/file/d/0B3tbeSUS2FsVOVlIamlDdkNBQUE/">here</a> .</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/big_brain_section.png"><img alt="_images/big_brain_section.png" src="_images/big_brain_section.png" style="width: 500.0px; height: 434.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 1</strong> : Big Brain section</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/ray.png"><img alt="_images/ray.png" src="_images/ray.png" style="width: 288.5px; height: 155.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 2</strong> : Ray extraction from segmentated cortex</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We will call <em>rays</em> the vectors of size 200 going from outside the brain and
through the cortex. As the images were stained for cell bodies, the intensity of each pixel of these rays represents the cell densities
and sizes contained in the cortical layer to which the pixel belongs. Since the 6 cortical layers
have different properties (cell density and size), the intensity profile can be used to
detect boundaries of the cortical layers.</p>
<p>Each ray has 2 input channels, one representing the smoothed intensity and the other,
the raw version, as shown in Figure 3. The next figure, Figure 4, shows the
ground truth segmentation map, where each different color represent
a different label. The purple color indicate that these pixels are
outside the cortex, while the 6 other colors represent the 6 cortical layers.
For example, the first layer of the cortex is between pixels ~ 35-55. The cortex
for this sample starts at pixel ~35 and ends at pixel ~170.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/raw_smooth.png"><img alt="_images/raw_smooth.png" src="_images/raw_smooth.png" style="width: 391.0px; height: 252.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 3</strong> : Raw and smooth intensity profiles (input channels)</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="_images/labels.png"><img alt="_images/labels.png" src="_images/labels.png" style="width: 372.0px; height: 76.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 4</strong> : Cortical layers labels for this ray</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this heading">¶</a></h2>
<p>We first started our experiment with more complex models, but we finally found that
the simpler model present here had enough capacity to learn how and where the layer boundaries are.
This model (depicted in Figure 5) is composed of 8 identical blocks, followed by a
last convolution and a softmax non linearity.</p>
<p>Each block is composed of :</p>
<ul class="simple">
<li><p>Batch Normalization layer</p></li>
<li><p>Rectify nonlinearity layer</p></li>
<li><p>Convolution layer, with kernel size 25, with enough padding such that the convolution does not change the feature resolution, and 64 features maps</p></li>
</ul>
<p>The last convolution has kernel size 1 and <em>number of classes</em> feature maps.
The softmax is then
used to detect which of these classes is more likely for each pixel.
Note that any input image size could be used here, since the model is built from
locally connected layers exclusively.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="_images/cortical_layers_net.png"><img alt="_images/cortical_layers_net.png" src="_images/cortical_layers_net.png" style="width: 242.0px; height: 762.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 5</strong> : Model</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Note that we didn’t use any pooling, because it was not needed. However, if
pooling layers were used, an upsampling path would have been necessary to recover full
spatial size of the input ray. Also, since each pixel of the output prediction has
a receptive field that includes all of the input pixel, the network is able to extract
enough contextual information.</p>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this heading">¶</a></h2>
<p>The model outputs a vector of the same size as the input (here, 200).
There are 7 class labels, including the 6 cortical layers and the ‘not in the brain yet’
label. You can see in Figure 6 below the output of the model for some ray. The top
of the plot represent the ground truth segmentation, while the bottoms represent
the predicted segmentation. As you can see, there is only a small number of pixels
not correctly segmented.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="_images/cortical_ray_result.png"><img alt="_images/cortical_ray_result.png" src="_images/cortical_ray_result.png" style="width: 372.0px; height: 118.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 6</strong> : Ground truth (top) vs prediction (bottom) for 1 ray</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>However, since the purpose was to do 3D segmentation by using 1D segmentation
of the rays, we needed to put back the rays on the brain section. After interpolation
between those rays and smoothing, we get the results shown in Figure 7. The colored
lines are from 3D meshes based on the prediction from the model, intersected with a 2D section, and the grayscale stripes correspond to the
ground truth. As you can see, it achieves really good results on the small manually labelled
sample, which extend well to previously unsegmented cortex.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="_images/cortical_valid1.png"><img alt="_images/cortical_valid1.png" src="_images/cortical_valid1.png" style="width: 586.4px; height: 375.6px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 7</strong> : Results put on the brain section</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="code">
<h2>Code<a class="headerlink" href="#code" title="Permalink to this heading">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ul class="simple">
<li><p>Current code works with Python 2 only.</p></li>
<li><p>If you use Theano with GPU backend (e.g. with Theano flag <code class="docutils literal notranslate"><span class="pre">device=cuda</span></code>),
you will need at least 12GB free in your video RAM.</p></li>
</ul>
</div>
<p>The FCN implementation can be found in the following file:</p>
<ul class="simple">
<li><p><a class="reference external" href="../code/cnn_1D_segm/fcn1D.py">fcn1D.py</a> : Main script. Defines the model.</p></li>
<li><p><a class="reference external" href="../code/cnn_1D_segm/train_fcn1D.py">train_fcn1D.py</a> : Training loop</p></li>
</ul>
<p>Change the <code class="docutils literal notranslate"><span class="pre">dataset_loaders/config.ini</span></code> file and add the right path for the dataset:</p>
<div class="highlight-cfg notranslate"><div class="highlight"><pre><span></span><span class="k">[cortical_layers]</span>
<span class="na">shared_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">/path/to/DeepLearningTutorials/data/cortical_layers/</span>
</pre></div>
</div>
<p>Folder indicated at section <code class="docutils literal notranslate"><span class="pre">[cortical_layers]</span></code> should contain a sub-folder named <code class="docutils literal notranslate"><span class="pre">6layers_segmentation</span></code>
(you can obtain it by just renaming the folder extracted from <code class="docutils literal notranslate"><span class="pre">TrainingData190417.tar.gz</span></code>) which should
itself contain files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">training_cls_indices.txt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">training_cls.txt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">training_geo.txt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">training_raw.txt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">training_regions.txt</span></code></p></li>
</ul>
<p>First define a <em>bn+relu+conv</em> block that returns the name of the last layer of
the block. Since the implementation uses a dictionary variable <em>net</em> that keeps
the layer’s name as key and the actual layer object as variable, the name of the
last layer is sufficient</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bn_relu_conv</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">incoming_layer</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">num_filters</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">pad</span> <span class="o">=</span> <span class="s1">&#39;same&#39;</span><span class="p">):</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)]</span> <span class="o">=</span> <span class="n">BatchNormLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">])</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;relu&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)]</span> <span class="o">=</span> <span class="n">NonlinearityLayer</span><span class="p">(</span> <span class="n">net</span><span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)],</span> <span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">rectify</span><span class="p">)</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;conv&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)]</span> <span class="o">=</span> <span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;relu&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)],</span>
                <span class="n">num_filters</span> <span class="o">=</span> <span class="n">num_filters</span><span class="p">,</span> <span class="n">filter_size</span> <span class="o">=</span> <span class="n">filter_size</span><span class="p">,</span>
                <span class="n">pad</span> <span class="o">=</span> <span class="n">pad</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">incoming_layer</span> <span class="o">=</span> <span class="s1">&#39;conv&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">incoming_layer</span>
</pre></div>
</div>
<p>The model is composed of 8 of these blocks, as seen below. Note that the
model implementation is very tweakable, since the depth (number of blocks), the
type of block, the filter size are the number of filters can all be changed by user.
However, the hyperparameters used here were:</p>
<ul class="simple">
<li><p>filter_size = 25</p></li>
<li><p>n_filters = 64</p></li>
<li><p>depth = 8</p></li>
<li><p>block = bn_relu_conv</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">input_var</span><span class="p">,</span>
	    <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
	    <span class="n">nb_in_channels</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">filter_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
        <span class="n">n_filters</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">depth</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">last_filter_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">block</span> <span class="o">=</span> <span class="s1">&#39;bn_relu_conv&#39;</span><span class="p">,</span>
        <span class="n">out_nonlin</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    input_var : theano 3Dtensor shape(n_samples, n_in_channels, ray_length)</span>
<span class="sd">    filter_size : odd int (to fit with same padding)</span>
<span class="sd">    n_filters : int, number of filters for each convLayer</span>
<span class="sd">    n_classes : int, number of classes to segment</span>
<span class="sd">    depth : int, number of stacked convolution before concatenation</span>
<span class="sd">    last_filter_size : int, last convolution filter size to obtain n_classes feature maps</span>
<span class="sd">    out_nonlin : default=softmax, non linearity function</span>
<span class="sd">    &#39;&#39;&#39;</span>


    <span class="n">net</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">InputLayer</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="n">nb_in_channels</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">input_var</span><span class="p">)</span>
    <span class="n">incoming_layer</span> <span class="o">=</span> <span class="s1">&#39;input&#39;</span>

    <span class="c1">#Convolution layers</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">block</span> <span class="o">==</span> <span class="s1">&#39;bn_relu_conv&#39;</span><span class="p">:</span>
            <span class="n">incoming_layer</span> <span class="o">=</span> <span class="n">bn_relu_conv</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">incoming_layer</span><span class="p">,</span> <span class="n">depth</span> <span class="o">=</span> <span class="n">d</span><span class="p">,</span>
                            <span class="n">num_filters</span><span class="o">=</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">filter_size</span><span class="o">=</span><span class="n">filter_size</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, the last convolution and softmax are achieved by :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="c1">#Output layer</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;final_conv&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">],</span>
                    <span class="n">num_filters</span> <span class="o">=</span> <span class="n">n_classes</span><span class="p">,</span>
                    <span class="n">filter_size</span> <span class="o">=</span> <span class="n">last_filter_size</span><span class="p">,</span>
                    <span class="n">pad</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span>
    <span class="n">incoming_layer</span> <span class="o">=</span> <span class="s1">&#39;final_conv&#39;</span>

    <span class="c1">#DimshuffleLayer and ReshapeLayer to fit the softmax implementation</span>
    <span class="c1">#(it needs a 1D or 2D tensor, not a 3D tensor)</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;final_dimshuffle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">DimshuffleLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">incoming_layer</span> <span class="o">=</span> <span class="s1">&#39;final_dimshuffle&#39;</span>

    <span class="n">layerSize</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;final_reshape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ReshapeLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">],</span>
                                <span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">layerSize</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]),</span><span class="n">layerSize</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
                                <span class="c1"># (200*batch_size,n_classes))</span>
    <span class="n">incoming_layer</span> <span class="o">=</span> <span class="s1">&#39;final_reshape&#39;</span>


    <span class="c1">#This is the layer that computes the prediction</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;last_layer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">NonlinearityLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">],</span>
                    <span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">out_nonlin</span><span class="p">)</span>
    <span class="n">incoming_layer</span> <span class="o">=</span> <span class="s1">&#39;last_layer&#39;</span>

    <span class="c1">#Layers needed to visualize the prediction of the network</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;probs_reshape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ReshapeLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">],</span>
                    <span class="p">(</span><span class="n">layerSize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layerSize</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_classes</span><span class="p">))</span>
    <span class="n">incoming_layer</span> <span class="o">=</span> <span class="s1">&#39;probs_reshape&#39;</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;probs_dimshuffle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">DimshuffleLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">incoming_layer</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>


    <span class="k">return</span> <span class="p">[</span><span class="n">net</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;last_layer&#39;</span><span class="p">]],</span> <span class="n">net</span>
</pre></div>
</div>
<p>Running <code class="docutils literal notranslate"><span class="pre">train_fcn1D.py</span></code> on a Titan X lasted for around 4 hours, ending with the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>THEANO_FLAGS=device=cuda0,floatX=float32,dnn.conv.algo_fwd=time_once,dnn.conv.algo_bwd_data=time_once,dnn.conv.algo_bwd_filter=time_once,gpuarray.preallocate=1 python train_fcn1D.py
[...]
EPOCH 412: Avg cost train 0.065615, acc train 0.993349, cost val 0.041758, acc val 0.984398, jacc val per class [&#39;0: 0.981183&#39;, &#39;1: 0.953546&#39;, &#39;2: 0.945765&#39;, &#39;3: 0.980471&#39;, &#39;4: 0.914617&#39;, &#39;5: 0.968710&#39;, &#39;6: 0.971049&#39;], jacc val 0.959335 took 31.422823 s
saving last model
</pre></div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<p>If you use this tutorial, please cite the following papers:</p>
<ul class="simple">
<li><p>References for BigBrain:</p>
<ul>
<li><p><a class="reference external" href="https://bigbrain.loris.ca/papers/HBM2014poster.pdf">[pdf]</a> Lewis, L.B. et al.: BigBrain: Initial Tissue Classification and Surface Extraction, HBM 2014.</p></li>
<li><p><a class="reference external" href="https://www.sciencemag.org/content/340/6139/1472.abstract">[website]</a> Amunts, K. et al.: “BigBrain: An Ultrahigh-Resolution 3D Human Brain Model”, Science (2013) 340 no. 6139 1472-1475, June 2013.</p></li>
<li><p><a class="reference external" href="https://bigbrain.loris.ca/papers/Poster-A0-OHBM-2012.pdf">[pdf]</a> Bludau, S. et al.: Two new Cytoarchitectonic Areas of the Human Frontal Pole, OHBM 2012.</p></li>
<li><p><a class="reference external" href="https://bigbrain.loris.ca/papers/HBM2010poster.pdf">[pdf]</a> Lepage, C. et al.: Automatic Repair of Acquisition Defects in Reconstruction of Histology Sections of a Human Brain, HBM 2010.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://github.com/fvisin/dataset_loaders">[GitHub Repo]</a> Francesco Visin, Adriana Romero - Dataset loaders: a python library to load and preprocess datasets. 2017</p></li>
</ul>
<p>Papers related to Theano/Lasagne:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1605.02688.pdf">[pdf]</a> Theano Development Team. Theano: A Python framework for fast computation of mathematical expresssions. May 2016.</p></li>
<li><p><a class="reference external" href="https://zenodo.org/record/27878#.WQocDrw18yc">[website]</a> Sander Dieleman, Jan Schluter, Colin Raffel, Eben Olson, Søren Kaae Sønderby, Daniel Nouri, Daniel Maturana, Martin Thoma, Eric Battenberg, Jack Kelly, Jeffrey De Fauw, Michael Heilman, diogo149, Brian McFee, Hendrik Weideman, takacsg84, peterderivaz, Jon, instagibbs, Dr. Kashif Rasul, CongLiu, Britefury, and Jonas Degrave, “Lasagne: First release.” (2015).</p></li>
</ul>
<section id="acknowledgements">
<h3>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permalink to this heading">¶</a></h3>
<p>This work was done in collaboration with Konrad Wagstyl, PhD student, University of Cambridge.
We would like to thank Professor Alan Evans’ <a class="reference external" href="https://www.mcin-cnim.ca">[MCIN lab]</a> and Professor Katrin Amunts’ <a class="reference external" href="https://www.fz-juelich.de/inm/inm-1/EN/Home/home_node.html">[INM-1 lab]</a>.</p>
<p>Thank you!</p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="contents.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Network for 1D segmentation</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#data">Data</a></li>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#results">Results</a></li>
<li><a class="reference internal" href="#code">Code</a></li>
<li><a class="reference internal" href="#references">References</a><ul>
<li><a class="reference internal" href="#acknowledgements">Acknowledgements</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="fcn_2D_segm.html"
                          title="previous chapter">Fully Convolutional Networks (FCN) for 2D segmentation</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="unet.html"
                          title="next chapter">U-Net</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/cnn_1D_segm.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="unet.html" title="U-Net"
             >next</a> |</li>
        <li class="right" >
          <a href="fcn_2D_segm.html" title="Fully Convolutional Networks (FCN) for 2D segmentation"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Network for 1D segmentation</a></li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2008--2010, LISA lab.
      Last updated on May 11, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.1.3.
    </div>
<script type="text/javascript">
  (function() {
    var ga = document.createElement('script');
    ga.src = ('https:' == document.location.protocol ?
              'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    ga.setAttribute('async', 'true');
    document.documentElement.firstChild.appendChild(ga);
  })();
</script>

  </body>
</html>