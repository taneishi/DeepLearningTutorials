
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>U-Net &#8212; DeepLearning 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinxdoc.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Network for 1D segmentation" href="cnn_1D_segm.html" />
 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-168290-9']);
  _gaq.push(['_trackPageview']);
</script>

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="cnn_1D_segm.html" title="Network for 1D segmentation"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">U-Net</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="u-net">
<span id="unet"></span><h1>U-Net<a class="headerlink" href="#u-net" title="Permalink to this heading">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section assumes the reader has already read through <a class="reference internal" href="lenet.html"><span class="doc">Convolutional Neural Networks (LeNet)</span></a> for
convolutional networks motivation and <a class="reference internal" href="fcn_2D_segm.html"><span class="doc">Fully Convolutional Networks (FCN) for 2D segmentation</span></a> for segmentation
network.</p>
</div>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h2>
<p>This tutorial provides a brief explanation of the U-Net architecture as well as a way to implement
it using Theano and Lasagne. U-Net is a Fully Convolutional Network (FCN) that does image segmentation.
Its goal is then to predict each pixel’s class. See <a class="reference internal" href="fcn_2D_segm.html"><span class="doc">Fully Convolutional Networks (FCN) for 2D segmentation</span></a> for differences between
network architecture for classification and segmentation tasks.</p>
</section>
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this heading">¶</a></h2>
<p>The data is from ISBI challenge and can be found <a class="reference external" href="http://brainiac2.mit.edu/isbi_challenge/home">here</a>.
We use data augmentation for training, as specified
in the defaults arguments in the code given below.</p>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this heading">¶</a></h2>
<p>The U-Net architecture is built upon the Fully Convolutional Network and modified
in a way that it yields better segmentation in medical imaging.
Compared to FCN-8, the two main differences are (1) U-net is symmetric and (2) the skip
connections between the downsampling path and the upsampling path apply a concatenation
operator instead of a sum. These skip connections intend to provide local information
to the global information while upsampling.
Because of its symmetry, the network has a large number of feature maps in the upsampling
path, which allows to transfer information. By comparison, the basic FCN architecture only had
<em>number of classes</em> feature maps in its upsampling path.</p>
<p>The U-Net owes its name to its symmetric shape, which is different from other FCN variants.</p>
<p>U-Net architecture is separated in 3 parts:</p>
<ul class="simple">
<li><p>1 : The contracting/downsampling path</p></li>
<li><p>2 : Bottleneck</p></li>
<li><p>3 : The expanding/upsampling path</p></li>
</ul>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/unet.jpg"><img alt="_images/unet.jpg" src="_images/unet.jpg" style="width: 540.0px; height: 405.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Figure 1</strong> : Illustration of U-Net architecture (from U-Net paper)</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="contracting-downsampling-path">
<h3>Contracting/downsampling path<a class="headerlink" href="#contracting-downsampling-path" title="Permalink to this heading">¶</a></h3>
<p>The contracting path is composed of 4 blocks. Each block is composed of</p>
<ul class="simple">
<li><p>3x3 Convolution Layer + activation function (with batch normalization)</p></li>
<li><p>3x3 Convolution Layer + activation function (with batch normalization)</p></li>
<li><p>2x2 Max Pooling</p></li>
</ul>
<p>Note that the number of feature maps doubles at each pooling, starting with
64 feature maps for the first block, 128 for the second, and so on.
The purpose of this contracting path is to capture the context of the input image
in order to be able to do segmentation. This coarse contextual information will
then be transfered to the upsampling path by means of skip connections.</p>
</section>
<section id="bottleneck">
<h3>Bottleneck<a class="headerlink" href="#bottleneck" title="Permalink to this heading">¶</a></h3>
<p>This part of the network is between the contracting and expanding paths.
The bottleneck is built from simply 2 convolutional layers (with batch
normalization), with dropout.</p>
</section>
<section id="expanding-upsampling-path">
<h3>Expanding/upsampling path<a class="headerlink" href="#expanding-upsampling-path" title="Permalink to this heading">¶</a></h3>
<p>The expanding path is also composed of 4 blocks. Each of these blocks is composed of</p>
<ul class="simple">
<li><p>Deconvolution layer with stride 2</p></li>
<li><p>Concatenation with the corresponding cropped feature map from the contracting path</p></li>
<li><p>3x3 Convolution layer + activation function (with batch normalization)</p></li>
<li><p>3x3 Convolution layer + activation function (with batch normalization)</p></li>
</ul>
<p>The purpose of this expanding path is to enable precise localization combined
with contextual information from the contracting path.</p>
</section>
<section id="advantages">
<h3>Advantages<a class="headerlink" href="#advantages" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>The U-Net combines the location information from the downsampling path with the contextual information in the upsampling path to finally obtain a general information combining localisation and context, which is necessary to predict a good segmentation map.</p></li>
<li><p>No dense layer, so images of different sizes can be used as input (since the only parameters to learn on convolution layers are the kernel, and the size of the kernel is independent from input image’ size).</p></li>
<li><p>The use of massive data augmentation is important in domains like biomedical segmentation, since the number of annotated samples is usually limited.</p></li>
</ul>
</section>
</section>
<section id="code">
<h2>Code<a class="headerlink" href="#code" title="Permalink to this heading">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ul class="simple">
<li><p>Current code works with Python 2 only.</p></li>
<li><p>If you use Theano with GPU backend (e.g. with Theano flag <code class="docutils literal notranslate"><span class="pre">device=cuda</span></code>),
you will need at least 12GB free in your video RAM.</p></li>
</ul>
</div>
<p>The U-Net implementation can be found in the following GitHub repo:</p>
<ul class="simple">
<li><p><a class="reference external" href="../code/unet/Unet_lasagne_recipes.py">Unet_lasagne_recipes.py</a>, from original main script
<a class="reference external" href="https://github.com/Lasagne/Recipes/blob/master/modelzoo/Unet.py">Unet.py</a>. Defines the model.</p></li>
<li><p><a class="reference external" href="../code/unet/train_unet.py">train_unet.py</a> : Training loop (main script to use).</p></li>
</ul>
<p>The user must install <a class="reference external" href="http://lasagne.readthedocs.io/en/latest/user/installation.html">Lasagne</a> ,
<a class="reference external" href="http://www.simpleitk.org/SimpleITK/resources/software.html">SimpleITK</a> and
clone the GitHub repo <a class="reference external" href="https://github.com/fvisin/dataset_loaders">Dataset Loaders</a>.</p>
<p>Change the <code class="docutils literal notranslate"><span class="pre">dataset_loaders/config.ini</span></code> file to set the right path for the dataset:</p>
<div class="highlight-cfg notranslate"><div class="highlight"><pre><span></span><span class="k">[isbi_em_stacks]</span>
<span class="na">shared_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">/path/to/DeepLearningTutorials/data/isbi_challenge_em_stacks/</span>
</pre></div>
</div>
<p>Folder indicated at section <code class="docutils literal notranslate"><span class="pre">[isbi_em_stacks]</span></code> should contain files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test-volume.tif</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train-labels.tif</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train-volume.tif</span></code></p></li>
</ul>
<p>The user can now build a U-Net with a specified number of input channels and number of classes.
First include the Lasagne layers needed to define the U-Net architecture :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">__author__</span> <span class="o">=</span> <span class="s1">&#39;Fabian Isensee&#39;</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">lasagne.layers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">InputLayer</span><span class="p">,</span> <span class="n">ConcatLayer</span><span class="p">,</span> <span class="n">Pool2DLayer</span><span class="p">,</span> <span class="n">ReshapeLayer</span><span class="p">,</span> <span class="n">DimshuffleLayer</span><span class="p">,</span> <span class="n">NonlinearityLayer</span><span class="p">,</span>
                            <span class="n">DropoutLayer</span><span class="p">,</span> <span class="n">Deconv2DLayer</span><span class="p">,</span> <span class="n">batch_norm</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">lasagne.layers.dnn</span> <span class="kn">import</span> <span class="n">Conv2DDNNLayer</span> <span class="k">as</span> <span class="n">ConvLayer</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">lasagne.layers</span> <span class="kn">import</span> <span class="n">Conv2DLayer</span> <span class="k">as</span> <span class="n">ConvLayer</span>
<span class="kn">import</span> <span class="nn">lasagne</span>
<span class="kn">from</span> <span class="nn">lasagne.init</span> <span class="kn">import</span> <span class="n">HeNormal</span>
</pre></div>
</div>
<p>The <em>net</em> variable will be an ordered dictionary containing layers names as keys and layers instances as value.
This is needed to be able to concatenate the feature maps from the contracting to expanding path.</p>
<p>First the contracting path :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_UNet</span><span class="p">(</span><span class="n">n_input_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_output_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">lasagne</span><span class="o">.</span><span class="n">nonlinearities</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">base_n_filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">do_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">InputLayer</span><span class="p">((</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">n_input_channels</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_1_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_1_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_1_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;pool1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pool2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_1_2&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_2_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;pool1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_2_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_2_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;pool2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pool2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_2_2&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_3_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;pool2&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_3_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_3_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;pool3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pool2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_3_2&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_4_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;pool3&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_4_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_4_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">net</span><span class="p">[</span><span class="s1">&#39;pool4&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pool2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_4_2&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>And then the bottleneck :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># the paper does not really describe where and how dropout is added. Feel free to try more options</span>
    <span class="k">if</span> <span class="n">do_dropout</span><span class="p">:</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">DropoutLayer</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;encode_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;encode_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;encode_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
</pre></div>
</div>
<p>Followed by the expanding path :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">Deconv2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;encode_2&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConcatLayer</span><span class="p">([</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale1&#39;</span><span class="p">],</span> <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_4_2&#39;</span><span class="p">]],</span> <span class="n">cropping</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_1_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_1_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_1_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">Deconv2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_1_2&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConcatLayer</span><span class="p">([</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale2&#39;</span><span class="p">],</span> <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_3_2&#39;</span><span class="p">]],</span> <span class="n">cropping</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_2_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat2&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_2_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_2_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">Deconv2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_2_2&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConcatLayer</span><span class="p">([</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale3&#39;</span><span class="p">],</span> <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_2_2&#39;</span><span class="p">]],</span> <span class="n">cropping</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_3_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat3&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_3_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_3_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>

    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale4&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">Deconv2DLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_3_2&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat4&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConcatLayer</span><span class="p">([</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;upscale4&#39;</span><span class="p">],</span> <span class="n">net</span><span class="p">[</span><span class="s1">&#39;contr_1_2&#39;</span><span class="p">]],</span> <span class="n">cropping</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="s2">&quot;center&quot;</span><span class="p">))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_4_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;concat4&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_4_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_4_1&#39;</span><span class="p">],</span> <span class="n">base_n_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)))</span>
</pre></div>
</div>
<p>And finally the output path (to obtain <em>number of classes</em> feature maps):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;output_segmentation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConvLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;expand_4_2&#39;</span><span class="p">],</span> <span class="n">num_output_classes</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;dimshuffle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">DimshuffleLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;output_segmentation&#39;</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;reshapeSeg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ReshapeLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;dimshuffle&#39;</span><span class="p">],</span> <span class="p">(</span><span class="n">num_output_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;dimshuffle2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">DimshuffleLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;reshapeSeg&#39;</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">net</span><span class="p">[</span><span class="s1">&#39;output_flattened&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">NonlinearityLayer</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="s1">&#39;dimshuffle2&#39;</span><span class="p">],</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">lasagne</span><span class="o">.</span><span class="n">nonlinearities</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">net</span>
</pre></div>
</div>
<p>Running <code class="docutils literal notranslate"><span class="pre">train_unet.py</span></code> on a Titan X lasted for around 60 minutes, ending with the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ THEANO_FLAGS=device=cuda0,floatX=float32,dnn.conv.algo_fwd=time_once,dnn.conv.algo_bwd_data=time_once,dnn.conv.algo_bwd_filter=time_once,gpuarray.preallocate=1 python train_unet.py
[...]
EPOCH 364: Avg epoch training cost train 0.160667, cost val 0.265909, acc val 0.888796, jacc val class 0  0.636058, jacc val class 1 0.861970, jacc val 0.749014 took 4.379772 s
</pre></div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<p>If you use this tutorial, please cite the following papers.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1505.04597.pdf">[pdf]</a> Olaf Ronneberger, Philipp Fischer, Thomas Brox. U_Net: Convolutional Networks for Biomedical Image Segmentation. May 2015.</p></li>
<li><p><a class="reference external" href="https://github.com/fvisin/dataset_loaders">[GitHub Repo]</a> Francesco Visin, Adriana Romero - Dataset loaders: a python library to load and preprocess datasets. 2017.</p></li>
</ul>
<p>Papers related to Theano/Lasagne:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1605.02688.pdf">[pdf]</a> Theano Development Team. Theano: A Python framework for fast computation of mathematical expresssions. May 2016.</p></li>
<li><p><a class="reference external" href="https://zenodo.org/record/27878#.WQocDrw18yc">[website]</a> Sander Dieleman, Jan Schluter, Colin Raffel, Eben Olson, Søren Kaae Sønderby, Daniel Nouri, Daniel Maturana, Martin Thoma, Eric Battenberg, Jack Kelly, Jeffrey De Fauw, Michael Heilman, diogo149, Brian McFee, Hendrik Weideman, takacsg84, peterderivaz, Jon, instagibbs, Dr. Kashif Rasul, CongLiu, Britefury, and Jonas Degrave, “Lasagne: First release.” (2015).</p></li>
</ul>
<p>Thank you!</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="contents.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">U-Net</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#data">Data</a></li>
<li><a class="reference internal" href="#model">Model</a><ul>
<li><a class="reference internal" href="#contracting-downsampling-path">Contracting/downsampling path</a></li>
<li><a class="reference internal" href="#bottleneck">Bottleneck</a></li>
<li><a class="reference internal" href="#expanding-upsampling-path">Expanding/upsampling path</a></li>
<li><a class="reference internal" href="#advantages">Advantages</a></li>
</ul>
</li>
<li><a class="reference internal" href="#code">Code</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="cnn_1D_segm.html"
                          title="previous chapter">Network for 1D segmentation</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/unet.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="cnn_1D_segm.html" title="Network for 1D segmentation"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">U-Net</a></li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2008--2010, LISA lab.
      Last updated on May 11, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.1.3.
    </div>
<script type="text/javascript">
  (function() {
    var ga = document.createElement('script');
    ga.src = ('https:' == document.location.protocol ?
              'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    ga.setAttribute('async', 'true');
    document.documentElement.firstChild.appendChild(ga);
  })();
</script>

  </body>
</html>